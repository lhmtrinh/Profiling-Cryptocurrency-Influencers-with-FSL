Training charlieoneill/distilbert-base-uncased-finetuned-tweet_eval-offensive
Some weights of the model checkpoint at charlieoneill/distilbert-base-uncased-finetuned-tweet_eval-offensive were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch 1/3
----------
Train loss 1.6065254759144139 accuracy 0.23857868020304568
Val loss   1.6036716057704046 accuracy 0.21

Epoch 2/3
----------
Train loss 1.5908839734824929 accuracy 0.32656514382402707
Val loss   1.5909269131146944 accuracy 0.22

Epoch 3/3
----------
Train loss 1.578774579473444 accuracy 0.32148900169204736
Val loss   1.5862608231030977 accuracy 0.27

Model saved to distilbert-base-uncased-finetuned-tweet_eval-offensive.pth

Training pig4431/TweetEval_roBERTa_5E
Some weights of the model checkpoint at pig4431/TweetEval_roBERTa_5E were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch 1/3
----------
Train loss 1.608823471778148 accuracy 0.20642978003384094
Val loss   1.6075008557393 accuracy 0.25

Epoch 2/3
----------
Train loss 1.6039855770162634 accuracy 0.21996615905245345
Val loss   1.5922767199002779 accuracy 0.3

Epoch 3/3
----------
Train loss 1.5881861416069236 accuracy 0.3147208121827411
Val loss   1.579892635345459 accuracy 0.29

Model saved to TweetEval_roBERTa_5E.pth

Training tner/twitter-roberta-base-dec2021-tweetner7-random
Some weights of the model checkpoint at tner/twitter-roberta-base-dec2021-tweetner7-random were not used when initializing RobertaModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch 1/3
----------
Train loss 1.6080322249515637 accuracy 0.21658206429780033
Val loss   1.6033155643022978 accuracy 0.22

Epoch 2/3
----------
Train loss 1.5964439269658681 accuracy 0.2470389170896785
Val loss   1.6021614166406484 accuracy 0.21

Epoch 3/3
----------
Train loss 1.590359394614761 accuracy 0.29949238578680204
Val loss   1.6007033861600435 accuracy 0.21

Model saved to twitter-roberta-base-dec2021-tweetner7-random.pth

Training cardiffnlp/tweet-topic-21-multi
Some weights of the model checkpoint at cardiffnlp/tweet-topic-21-multi were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch 1/3
----------
Train loss 1.6053034969278284 accuracy 0.2233502538071066
Val loss   1.5970577093271108 accuracy 0.28

Epoch 2/3
----------
Train loss 1.5767696934777338 accuracy 0.3519458544839255
Val loss   1.5532192358603845 accuracy 0.35000000000000003

Epoch 3/3
----------
Train loss 1.537568892981555 accuracy 0.4196277495769881
Val loss   1.5368998234088604 accuracy 0.35000000000000003

Model saved to tweet-topic-21-multi.pth

Training cardiffnlp/twitter-roberta-base-2021-124m
Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-2021-124m were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch 1/3
----------
Train loss 1.5984258442311674 accuracy 0.27918781725888325
Val loss   1.5937675879551814 accuracy 0.31

Epoch 2/3
----------
Train loss 1.564798662791381 accuracy 0.38409475465313025
Val loss   1.5427388686400194 accuracy 0.35000000000000003

Epoch 3/3
----------
Train loss 1.5182983375884391 accuracy 0.4128595600676819
Val loss   1.5205105451437144 accuracy 0.36

Model saved to twitter-roberta-base-2021-124m.pth

