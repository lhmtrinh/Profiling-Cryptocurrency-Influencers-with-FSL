Some weights of the model checkpoint at charlieoneill/distilbert-base-uncased-finetuned-tweet_eval-offensive were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at pig4431/TweetEval_roBERTa_5E were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at pig4431/TweetEval_roBERTa_5E and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at tner/twitter-roberta-base-dec2021-tweetner7-random were not used when initializing RobertaModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at tner/twitter-roberta-base-dec2021-tweetner7-random and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at cardiffnlp/tweet-topic-21-multi were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/tweet-topic-21-multi and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-2021-124m were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-2021-124m and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training charlieoneill/distilbert-base-uncased-finetuned-tweet_eval-offensive
Epoch 1/4
----------
Train loss 1.608386676911025 accuracy 0.22466960352422907
Val loss   1.6008018791675567 accuracy 0.34177215189873417

Epoch 2/4
----------
Train loss 1.5943942411601195 accuracy 0.3142437591776799
Val loss   1.5814288318157197 accuracy 0.31645569620253167

Epoch 3/4
----------
Train loss 1.5508803603244803 accuracy 0.4199706314243759
Val loss   1.5227584958076477 accuracy 0.3291139240506329

Epoch 4/4
----------
Train loss 1.515484943027385 accuracy 0.44933920704845814
Val loss   1.5060346722602844 accuracy 0.3291139240506329

Model saved to distilbert-base-uncased-finetuned-tweet_eval-offensive.pth

Training pig4431/TweetEval_roBERTa_5E
Epoch 1/4
----------
Train loss 1.6107097334331937 accuracy 0.17621145374449337
Val loss   1.6094379425048828 accuracy 0.13924050632911392

Epoch 2/4
----------
Train loss 1.608950394636009 accuracy 0.15124816446402348
Val loss   1.6094379425048828 accuracy 0.13924050632911392

Epoch 3/4
----------
Train loss 1.6060074623565228 accuracy 0.2158590308370044
Val loss   1.6011389315128326 accuracy 0.2911392405063291

Epoch 4/4
----------
Train loss 1.596536724190963 accuracy 0.2672540381791483
Val loss   1.5821278870105744 accuracy 0.5063291139240507

Model saved to TweetEval_roBERTa_5E.pth

Training tner/twitter-roberta-base-dec2021-tweetner7-random
Epoch 1/4
----------
Train loss 1.6091703399580124 accuracy 0.21145374449339205
Val loss   1.6077523112297059 accuracy 0.34177215189873417

Epoch 2/4
----------
Train loss 1.5828700630288375 accuracy 0.33480176211453744
Val loss   1.5511363387107848 accuracy 0.34177215189873417

Epoch 3/4
----------
Train loss 1.5263285581131427 accuracy 0.4229074889867841
Val loss   1.5168592929840088 accuracy 0.31645569620253167

Epoch 4/4
----------
Train loss 1.4857597706610697 accuracy 0.4698972099853157
Val loss   1.5065049827098846 accuracy 0.3291139240506329

Model saved to twitter-roberta-base-dec2021-tweetner7-random.pth

Training cardiffnlp/tweet-topic-21-multi
Epoch 1/4
----------
Train loss 1.6086059097658123 accuracy 0.21145374449339205
Val loss   1.6057468354701996 accuracy 0.24050632911392406

Epoch 2/4
----------
Train loss 1.5846546302761948 accuracy 0.31277533039647576
Val loss   1.5672378063201904 accuracy 0.3670886075949367

Epoch 3/4
----------
Train loss 1.5336185205749602 accuracy 0.4052863436123348
Val loss   1.489529448747635 accuracy 0.46835443037974683

Epoch 4/4
----------
Train loss 1.4725606748235156 accuracy 0.5168869309838473
Val loss   1.4575778722763062 accuracy 0.45569620253164556

Model saved to tweet-topic-21-multi.pth

Training cardiffnlp/twitter-roberta-base-2021-124m
Epoch 1/4
----------
Train loss 1.607943076139305 accuracy 0.2173274596182085
Val loss   1.6078052520751953 accuracy 0.22784810126582278

Epoch 2/4
----------
Train loss 1.5962936271700943 accuracy 0.3039647577092511
Val loss   1.5692173659801483 accuracy 0.31645569620253167

Epoch 3/4
----------
Train loss 1.5405581541228712 accuracy 0.41556534508076354
Val loss   1.4921241223812103 accuracy 0.43037974683544306

Epoch 4/4
----------
Train loss 1.4929590378588402 accuracy 0.46549192364170333
Val loss   1.4619281947612763 accuracy 0.4810126582278481

Model saved to twitter-roberta-base-2021-124m.pth

