{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetModel, XLNetTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "RANDOM_SEED = 10\n",
    "MODEL = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(MODEL)\n",
    "NUMBER_LABELS = 5 \n",
    "BATCH_SIZE = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"../data/processed_data_v3.csv\")[[\"Q2_Q\",\"Q2_A\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Q2_A = df.Q2_A-1\n",
    "df.Q2_A.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pytorch dataset class and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFG_dataset(Dataset):\n",
    "\n",
    "  def __init__(self, answers, scores, tokenizer, max_len):\n",
    "    self.answers = answers\n",
    "    self.scores = scores\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.answers)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    answer = self.answers[idx]\n",
    "    score = self.scores[idx]\n",
    "\n",
    "    encoding = tokenizer(\n",
    "      answer,\n",
    "      max_length = self.max_len,\n",
    "      padding ='max_length',\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'answer_text': answer,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'score': torch.tensor(score, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size=1):\n",
    "  ds = LFG_dataset(\n",
    "    answers=df.Q2_Q.to_numpy(),\n",
    "    scores=df.Q2_A.to_numpy().astype(int),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: find class centroids. Take only the centroids of abundant classes such as 2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "df_train, df_val = train_test_split(df_train_val, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, 600, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, 600, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFG_grading(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LFG_grading, self).__init__()\n",
    "        self.xlnet = XLNetModel.from_pretrained(MODEL)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(self.xlnet.config.hidden_size, 200)\n",
    "        self.fc2 = nn.Linear(200,200)\n",
    "        self.fc3 = nn.Linear(200,n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.xlnet(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        # Get the first element of output which is the hidden state\n",
    "        # Get the embeddings of CLS token\n",
    "        # if u use BERT please make sure to change from -1 to 0\n",
    "        cls_embeddings = output[0][:,-1,:]\n",
    "        output = self.drop(cls_embeddings)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.drop(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.drop(output)\n",
    "        output = self.fc3(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "    def requires_grad_embeddings(self, val):\n",
    "        for param in self.xlnet.parameters():\n",
    "            param.requires_grad = val   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrdinalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        loss = criterion(output, target).sum(axis=1)\n",
    "        return loss.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "model = LFG_grading(5).to(device)\n",
    "model.requires_grad_embeddings(False)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for i,d in enumerate(tqdm(data_loader)):\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"score\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    \n",
    "    if i>= 3:\n",
    "      model.requires_grad_embeddings(True)\n",
    "\n",
    "    # loss function\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "    correct_predictions += torch.sum(predictions == targets)\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, data_loader, device, n_example):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    predictions = torch.tensor([], dtype=torch.long).to(device)\n",
    "    targets = torch.tensor([], dtype=torch.long).to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    for i,d in enumerate(tqdm(data_loader)):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        score = d[\"score\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        predictions = torch.cat((predictions,outputs.argmax(dim=1)), dim=0)\n",
    "        targets = torch.cat((targets,score), dim=0)\n",
    "        \n",
    "    correct_predictions = torch.sum(predictions == targets)\n",
    "        \n",
    "    return (correct_predictions.double() / n_example), targets, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "train_acc= []\n",
    "train_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  acc, loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "  \n",
    "  train_acc.append(acc)\n",
    "  train_loss.append(loss)\n",
    "  print(f'Train loss {loss} accuracy {acc}')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc, true, predict = val(model, val_data_loader, device, len(df_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
