{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhmtr\\.conda\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 10\n",
    "MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "NUMBER_LABELS = 5 \n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"../data/processed_data_v3.csv\")[[\"Q2_Q\",\"Q2_A\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q2_Q</th>\n",
       "      <th>Q2_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent education to me is the type of educa...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Excellent Education is the process of knowl...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An excellent education is an education given t...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patty Murray once said, \"Good education learni...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An excellent education is one that exposes a s...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>An excellent education is something that bring...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>Excellent education in my opinion prepares the...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>An excellent education is an education given t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>Education according to the Oxford dictionary i...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>An excellent education to me is when all peopl...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2037 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Q2_Q  Q2_A\n",
       "0     Excellent education to me is the type of educa...   4.0\n",
       "1     An Excellent Education is the process of knowl...   3.0\n",
       "2     An excellent education is an education given t...   4.0\n",
       "3     Patty Murray once said, \"Good education learni...   4.0\n",
       "4     An excellent education is one that exposes a s...   2.0\n",
       "...                                                 ...   ...\n",
       "3112  An excellent education is something that bring...   3.0\n",
       "3114  Excellent education in my opinion prepares the...   3.0\n",
       "3119  An excellent education is an education given t...   2.0\n",
       "3140  Education according to the Oxford dictionary i...   3.0\n",
       "3144  An excellent education to me is when all peopl...   3.0\n",
       "\n",
       "[2037 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform labels for ordinal regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_Q2_A = []\n",
    "for score in df.Q2_A:\n",
    "    transformed = np.zeros(NUMBER_LABELS)\n",
    "    idx = int(score)\n",
    "    transformed[:idx] = np.ones(idx)\n",
    "    transformed_Q2_A.append(transformed)\n",
    "\n",
    "df.Q2_A = transformed_Q2_A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pytorch dataset class and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFG_dataset(Dataset):\n",
    "\n",
    "  def __init__(self, answers, scores, tokenizer, max_len):\n",
    "    self.answers = answers\n",
    "    self.scores = scores\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.answers)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    answer = self.answers[idx]\n",
    "    score = self.scores[idx]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      answer,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'answer_text': answer,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'score': torch.tensor(score, dtype=torch.float32)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = LFG_dataset(\n",
    "    answers=df.Q2_Q.to_numpy(),\n",
    "    scores=df.Q2_A.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, 512, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, 512, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answer_text', 'input_ids', 'attention_mask', 'score'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['score']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFG_grading(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(LFG_grading, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(MODEL)\n",
    "    # Freeze embeddings of BERT\n",
    "    for param in self.bert.parameters():\n",
    "      param.requires_grad = False\n",
    "    self.drop = nn.Dropout(p=0.5)\n",
    "    self.fc1 = nn.Linear(self.bert.config.hidden_size, 200)\n",
    "    self.fc2 = nn.Linear(200,200)\n",
    "    self.fc3 = nn.Linear(200,n_classes)\n",
    "    self.sigmoid = nn.Softmax()\n",
    "    self.relu = nn.ReLU()\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    # Get the first element of output which is the hidden state\n",
    "    # Get the embeddings of first token (CLS token)\n",
    "    cls_embeddings = output[0][:,0,:]\n",
    "    output = self.drop(cls_embeddings)\n",
    "    output = self.fc1(output)\n",
    "    output = self.relu(output)\n",
    "    output = self.drop(output)\n",
    "    output = self.fc2(output)\n",
    "    output = self.relu(output)\n",
    "    output = self.drop(output)\n",
    "    output = self.fc3(output)\n",
    "    return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction2label(pred: np.ndarray):\n",
    "    \"\"\"Convert ordinal predictions to class labels, e.g.\n",
    "    \n",
    "    [0.9, 0.1, 0.1, 0.1] -> 1\n",
    "    [0.9, 0.9, 0.1, 0.1] -> 2\n",
    "    [0.9, 0.9, 0.9, 0.1] -> 3\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    return (pred > 0.5).cumprod(axis=1).sum(axis=1)\n",
    "\n",
    "\n",
    "class OrdinalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrdinalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"Ordinal regression with encoding as in https://arxiv.org/pdf/0704.1028.pdf\"\"\"\n",
    "\n",
    "        # Take MSE loss of one hot encodings vector\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        loss = criterion(output, target).sum(axis=1)\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run of model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LFG_grading(5)\n",
    "model = model.to(device)\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "predictions = model(input_ids, attention_mask)\n",
    "targets = data[\"score\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = OrdinalLoss()\n",
    "loss = criterion(predictions,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(prediction2label(predictions) == prediction2label(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2label(predictions)\n",
    "prediction2label(targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "model = LFG_grading(5).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = OrdinalLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"score\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    # loss function\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    predictions = prediction2label(outputs)\n",
    "    targets = prediction2label(targets)\n",
    "    correct_predictions += torch.sum(predictions == targets)\n",
    "  \n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.3447899618112384 accuracy 4.3339374357738025\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.36224768139661756 accuracy 2.81546472885953\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.3720676486633933 accuracy 2.6418205410323288\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.36879432624113473 accuracy 2.627136587122156\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.3753409710856519 accuracy 2.5851127990909752\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.381342062193126 accuracy 2.5580704986886693\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.3720676486633933 accuracy 2.560966005767074\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.39498090561920346 accuracy 2.5461770117445277\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.4069830878341516 accuracy 2.52199354808402\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.38079650845608287 accuracy 2.540116934912731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "train_acc= []\n",
    "train_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  acc, loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "  train_acc.append(acc)\n",
    "  train_loss.append(loss)\n",
    "  print(f'Train loss {acc} accuracy {loss}')\n",
    "  print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
