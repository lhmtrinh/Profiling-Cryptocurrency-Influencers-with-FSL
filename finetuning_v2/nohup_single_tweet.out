Training roberta-large:

Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]
Downloading (…)lve/main/config.json: 100%|██████████| 482/482 [00:00<00:00, 116kB/s]

Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.30MB/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.29MB/s]

Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.69MB/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.68MB/s]

Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 13.5MB/s]

Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]
Downloading pytorch_model.bin:   1%|          | 10.5M/1.43G [00:00<00:30, 46.2MB/s]
Downloading pytorch_model.bin:   2%|▏         | 31.5M/1.43G [00:00<00:14, 97.2MB/s]
Downloading pytorch_model.bin:   4%|▍         | 62.9M/1.43G [00:00<00:08, 166MB/s] 
Downloading pytorch_model.bin:   7%|▋         | 94.4M/1.43G [00:00<00:06, 202MB/s]
Downloading pytorch_model.bin:  10%|▉         | 136M/1.43G [00:00<00:05, 241MB/s] 
Downloading pytorch_model.bin:  13%|█▎        | 178M/1.43G [00:00<00:04, 278MB/s]
Downloading pytorch_model.bin:  15%|█▌        | 220M/1.43G [00:00<00:04, 293MB/s]
Downloading pytorch_model.bin:  18%|█▊        | 262M/1.43G [00:01<00:03, 307MB/s]
Downloading pytorch_model.bin:  21%|██▏       | 304M/1.43G [00:01<00:03, 325MB/s]
Downloading pytorch_model.bin:  24%|██▍       | 346M/1.43G [00:01<00:03, 326MB/s]
Downloading pytorch_model.bin:  27%|██▋       | 388M/1.43G [00:01<00:03, 340MB/s]
Downloading pytorch_model.bin:  30%|███       | 430M/1.43G [00:01<00:02, 354MB/s]
Downloading pytorch_model.bin:  33%|███▎      | 472M/1.43G [00:01<00:03, 318MB/s]
Downloading pytorch_model.bin:  36%|███▌      | 514M/1.43G [00:01<00:03, 300MB/s]
Downloading pytorch_model.bin:  39%|███▉      | 556M/1.43G [00:01<00:02, 307MB/s]
Downloading pytorch_model.bin:  41%|████      | 587M/1.43G [00:02<00:02, 292MB/s]
Downloading pytorch_model.bin:  44%|████▍     | 629M/1.43G [00:02<00:02, 308MB/s]
Downloading pytorch_model.bin:  46%|████▋     | 661M/1.43G [00:02<00:02, 294MB/s]
Downloading pytorch_model.bin:  49%|████▊     | 692M/1.43G [00:02<00:02, 294MB/s]
Downloading pytorch_model.bin:  51%|█████▏    | 734M/1.43G [00:02<00:02, 310MB/s]
Downloading pytorch_model.bin:  54%|█████▍    | 776M/1.43G [00:02<00:01, 334MB/s]
Downloading pytorch_model.bin:  57%|█████▋    | 818M/1.43G [00:02<00:01, 347MB/s]
Downloading pytorch_model.bin:  60%|██████    | 860M/1.43G [00:02<00:01, 343MB/s]
Downloading pytorch_model.bin:  63%|██████▎   | 902M/1.43G [00:03<00:01, 343MB/s]
Downloading pytorch_model.bin:  66%|██████▌   | 944M/1.43G [00:03<00:01, 341MB/s]
Downloading pytorch_model.bin:  69%|██████▉   | 986M/1.43G [00:03<00:01, 320MB/s]
Downloading pytorch_model.bin:  72%|███████▏  | 1.03G/1.43G [00:03<00:01, 327MB/s]
Downloading pytorch_model.bin:  75%|███████▌  | 1.07G/1.43G [00:03<00:01, 343MB/s]
Downloading pytorch_model.bin:  78%|███████▊  | 1.11G/1.43G [00:03<00:00, 351MB/s]
Downloading pytorch_model.bin:  81%|████████  | 1.15G/1.43G [00:03<00:00, 345MB/s]
Downloading pytorch_model.bin:  84%|████████▍ | 1.20G/1.43G [00:03<00:00, 351MB/s]
Downloading pytorch_model.bin:  87%|████████▋ | 1.24G/1.43G [00:04<00:00, 321MB/s]
Downloading pytorch_model.bin:  90%|████████▉ | 1.28G/1.43G [00:04<00:00, 323MB/s]
Downloading pytorch_model.bin:  93%|█████████▎| 1.32G/1.43G [00:04<00:00, 329MB/s]
Downloading pytorch_model.bin:  96%|█████████▌| 1.36G/1.43G [00:04<00:00, 345MB/s]
Downloading pytorch_model.bin:  99%|█████████▊| 1.41G/1.43G [00:04<00:00, 335MB/s]
Downloading pytorch_model.bin: 100%|██████████| 1.43G/1.43G [00:04<00:00, 308MB/s]
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.523, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.44145825505256653, 'eval_f1': 0.3063063063063063, 'eval_roc_auc': 0.5838607594936709, 'eval_accuracy': 0.17721518987341772, 'eval_runtime': 3.8961, 'eval_samples_per_second': 20.277, 'eval_steps_per_second': 5.133, 'epoch': 1.0}
{'loss': 0.4055, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.43166327476501465, 'eval_f1': 0.4583333333333333, 'eval_roc_auc': 0.6582278481012659, 'eval_accuracy': 0.4177215189873418, 'eval_runtime': 3.6194, 'eval_samples_per_second': 21.827, 'eval_steps_per_second': 5.526, 'epoch': 2.0}
{'loss': 0.3631, 'learning_rate': 1.902958579881657e-05, 'epoch': 2.98}
{'eval_loss': 0.3922716975212097, 'eval_f1': 0.5294117647058824, 'eval_roc_auc': 0.694620253164557, 'eval_accuracy': 0.45569620253164556, 'eval_runtime': 3.6154, 'eval_samples_per_second': 21.851, 'eval_steps_per_second': 5.532, 'epoch': 3.0}
{'loss': 0.2726, 'learning_rate': 1.8627218934911243e-05, 'epoch': 3.98}
{'eval_loss': 0.38701167702674866, 'eval_f1': 0.6103896103896105, 'eval_roc_auc': 0.7531645569620253, 'eval_accuracy': 0.5822784810126582, 'eval_runtime': 3.6149, 'eval_samples_per_second': 21.854, 'eval_steps_per_second': 5.533, 'epoch': 4.0}
{'loss': 0.2052, 'learning_rate': 1.822485207100592e-05, 'epoch': 4.97}
{'eval_loss': 0.6126995086669922, 'eval_f1': 0.5350318471337578, 'eval_roc_auc': 0.7088607594936709, 'eval_accuracy': 0.5316455696202531, 'eval_runtime': 3.6188, 'eval_samples_per_second': 21.831, 'eval_steps_per_second': 5.527, 'epoch': 5.0}
{'loss': 0.1375, 'learning_rate': 1.7822485207100592e-05, 'epoch': 5.96}
{'eval_loss': 0.7122216820716858, 'eval_f1': 0.5290322580645163, 'eval_roc_auc': 0.7041139240506329, 'eval_accuracy': 0.5189873417721519, 'eval_runtime': 3.615, 'eval_samples_per_second': 21.854, 'eval_steps_per_second': 5.533, 'epoch': 6.0}
{'train_runtime': 745.0071, 'train_samples_per_second': 45.704, 'train_steps_per_second': 11.476, 'train_loss': 0.31720743042218985, 'epoch': 6.0}

Training google/electra-large-discriminator:

Downloading (…)okenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]
Downloading (…)okenizer_config.json: 100%|██████████| 27.0/27.0 [00:00<00:00, 20.2kB/s]

Downloading (…)lve/main/config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]
Downloading (…)lve/main/config.json: 100%|██████████| 668/668 [00:00<00:00, 520kB/s]

Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]
Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 65.4MB/s]

Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.73MB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.72MB/s]

Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]
Downloading pytorch_model.bin:   1%|          | 10.5M/1.34G [00:00<01:45, 12.6MB/s]
Downloading pytorch_model.bin:   2%|▏         | 21.0M/1.34G [00:01<01:03, 20.9MB/s]
Downloading pytorch_model.bin:   2%|▏         | 31.5M/1.34G [00:01<00:45, 29.0MB/s]
Downloading pytorch_model.bin:   3%|▎         | 41.9M/1.34G [00:01<00:40, 32.1MB/s]
Downloading pytorch_model.bin:   4%|▍         | 52.4M/1.34G [00:01<00:38, 33.9MB/s]
Downloading pytorch_model.bin:   5%|▍         | 62.9M/1.34G [00:02<00:36, 35.1MB/s]
Downloading pytorch_model.bin:   5%|▌         | 73.4M/1.34G [00:02<00:32, 39.7MB/s]
Downloading pytorch_model.bin:   6%|▌         | 83.9M/1.34G [00:02<00:32, 39.3MB/s]
Downloading pytorch_model.bin:   7%|▋         | 94.4M/1.34G [00:02<00:32, 38.9MB/s]
Downloading pytorch_model.bin:   8%|▊         | 105M/1.34G [00:03<00:32, 38.5MB/s] 
Downloading pytorch_model.bin:   9%|▊         | 115M/1.34G [00:03<00:29, 42.2MB/s]
Downloading pytorch_model.bin:   9%|▉         | 126M/1.34G [00:03<00:29, 41.0MB/s]
Downloading pytorch_model.bin:  10%|█         | 136M/1.34G [00:03<00:27, 44.0MB/s]
Downloading pytorch_model.bin:  11%|█         | 147M/1.34G [00:04<00:28, 42.2MB/s]
Downloading pytorch_model.bin:  12%|█▏        | 157M/1.34G [00:04<00:28, 41.0MB/s]
Downloading pytorch_model.bin:  12%|█▏        | 168M/1.34G [00:04<00:27, 43.5MB/s]
Downloading pytorch_model.bin:  13%|█▎        | 178M/1.34G [00:04<00:27, 42.2MB/s]
Downloading pytorch_model.bin:  14%|█▍        | 189M/1.34G [00:05<00:31, 36.7MB/s]
Downloading pytorch_model.bin:  15%|█▍        | 199M/1.34G [00:05<00:31, 36.6MB/s]
Downloading pytorch_model.bin:  16%|█▌        | 210M/1.34G [00:05<00:30, 36.9MB/s]
Downloading pytorch_model.bin:  16%|█▋        | 220M/1.34G [00:06<00:30, 37.0MB/s]
Downloading pytorch_model.bin:  17%|█▋        | 231M/1.34G [00:06<00:31, 35.3MB/s]
Downloading pytorch_model.bin:  18%|█▊        | 241M/1.34G [00:06<00:30, 36.2MB/s]
Downloading pytorch_model.bin:  19%|█▊        | 252M/1.34G [00:07<00:32, 33.6MB/s]
Downloading pytorch_model.bin:  19%|█▉        | 262M/1.34G [00:07<00:33, 32.0MB/s]
Downloading pytorch_model.bin:  20%|██        | 273M/1.34G [00:07<00:34, 31.0MB/s]
Downloading pytorch_model.bin:  21%|██        | 283M/1.34G [00:08<00:34, 30.3MB/s]
Downloading pytorch_model.bin:  22%|██▏       | 294M/1.34G [00:08<00:35, 29.8MB/s]
Downloading pytorch_model.bin:  23%|██▎       | 304M/1.34G [00:08<00:35, 29.7MB/s]
Downloading pytorch_model.bin:  23%|██▎       | 315M/1.34G [00:09<00:32, 31.6MB/s]
Downloading pytorch_model.bin:  24%|██▍       | 325M/1.34G [00:09<00:33, 30.8MB/s]
Downloading pytorch_model.bin:  25%|██▍       | 336M/1.34G [00:09<00:33, 30.3MB/s]
Downloading pytorch_model.bin:  26%|██▌       | 346M/1.34G [00:10<00:33, 30.2MB/s]
Downloading pytorch_model.bin:  27%|██▋       | 357M/1.34G [00:10<00:31, 31.7MB/s]
Downloading pytorch_model.bin:  27%|██▋       | 367M/1.34G [00:10<00:31, 31.0MB/s]
Downloading pytorch_model.bin:  28%|██▊       | 377M/1.34G [00:11<00:29, 32.6MB/s]
Downloading pytorch_model.bin:  29%|██▉       | 388M/1.34G [00:11<00:30, 31.5MB/s]
Downloading pytorch_model.bin:  30%|██▉       | 398M/1.34G [00:11<00:30, 30.9MB/s]
Downloading pytorch_model.bin:  30%|███       | 409M/1.34G [00:12<00:28, 32.6MB/s]
Downloading pytorch_model.bin:  31%|███       | 419M/1.34G [00:12<00:29, 31.6MB/s]
Downloading pytorch_model.bin:  32%|███▏      | 430M/1.34G [00:12<00:27, 33.0MB/s]
Downloading pytorch_model.bin:  33%|███▎      | 440M/1.34G [00:13<00:28, 32.0MB/s]
Downloading pytorch_model.bin:  34%|███▎      | 451M/1.34G [00:13<00:26, 33.3MB/s]
Downloading pytorch_model.bin:  34%|███▍      | 461M/1.34G [00:13<00:27, 31.8MB/s]
Downloading pytorch_model.bin:  35%|███▌      | 472M/1.34G [00:14<00:26, 33.3MB/s]
Downloading pytorch_model.bin:  36%|███▌      | 482M/1.34G [00:14<00:26, 32.0MB/s]
Downloading pytorch_model.bin:  37%|███▋      | 493M/1.34G [00:14<00:25, 33.4MB/s]
Downloading pytorch_model.bin:  37%|███▋      | 503M/1.34G [00:15<00:26, 31.9MB/s]
Downloading pytorch_model.bin:  38%|███▊      | 514M/1.34G [00:15<00:24, 33.3MB/s]
Downloading pytorch_model.bin:  39%|███▉      | 524M/1.34G [00:15<00:25, 31.9MB/s]
Downloading pytorch_model.bin:  40%|███▉      | 535M/1.34G [00:15<00:24, 33.3MB/s]
Downloading pytorch_model.bin:  41%|████      | 545M/1.34G [00:16<00:24, 32.1MB/s]
Downloading pytorch_model.bin:  41%|████▏     | 556M/1.34G [00:16<00:23, 33.5MB/s]
Downloading pytorch_model.bin:  42%|████▏     | 566M/1.34G [00:16<00:24, 32.1MB/s]
Downloading pytorch_model.bin:  43%|████▎     | 577M/1.34G [00:17<00:22, 33.5MB/s]
Downloading pytorch_model.bin:  44%|████▎     | 587M/1.34G [00:17<00:23, 32.1MB/s]
Downloading pytorch_model.bin:  44%|████▍     | 598M/1.34G [00:17<00:22, 33.6MB/s]
Downloading pytorch_model.bin:  45%|████▌     | 608M/1.34G [00:18<00:22, 32.2MB/s]
Downloading pytorch_model.bin:  46%|████▌     | 619M/1.34G [00:18<00:22, 32.6MB/s]
Downloading pytorch_model.bin:  47%|████▋     | 629M/1.34G [00:18<00:22, 32.2MB/s]
Downloading pytorch_model.bin:  48%|████▊     | 640M/1.34G [00:19<00:21, 33.1MB/s]
Downloading pytorch_model.bin:  48%|████▊     | 650M/1.34G [00:19<00:21, 32.4MB/s]
Downloading pytorch_model.bin:  49%|████▉     | 661M/1.34G [00:19<00:21, 32.6MB/s]
Downloading pytorch_model.bin:  50%|████▉     | 671M/1.34G [00:20<00:20, 32.5MB/s]
Downloading pytorch_model.bin:  51%|█████     | 682M/1.34G [00:20<00:19, 33.7MB/s]
Downloading pytorch_model.bin:  51%|█████▏    | 692M/1.34G [00:20<00:20, 32.4MB/s]
Downloading pytorch_model.bin:  52%|█████▏    | 703M/1.34G [00:21<00:19, 33.8MB/s]
Downloading pytorch_model.bin:  53%|█████▎    | 713M/1.34G [00:21<00:19, 32.4MB/s]
Downloading pytorch_model.bin:  54%|█████▍    | 724M/1.34G [00:21<00:18, 33.5MB/s]
Downloading pytorch_model.bin:  55%|█████▍    | 734M/1.34G [00:22<00:18, 32.8MB/s]
Downloading pytorch_model.bin:  55%|█████▌    | 744M/1.34G [00:22<00:17, 33.7MB/s]
Downloading pytorch_model.bin:  56%|█████▌    | 755M/1.34G [00:22<00:17, 33.8MB/s]
Downloading pytorch_model.bin:  57%|█████▋    | 765M/1.34G [00:22<00:17, 33.3MB/s]
Downloading pytorch_model.bin:  58%|█████▊    | 776M/1.34G [00:23<00:16, 33.7MB/s]
Downloading pytorch_model.bin:  58%|█████▊    | 786M/1.34G [00:23<00:17, 32.7MB/s]
Downloading pytorch_model.bin:  59%|█████▉    | 797M/1.34G [00:23<00:16, 33.6MB/s]
Downloading pytorch_model.bin:  60%|██████    | 807M/1.34G [00:24<00:15, 35.0MB/s]
Downloading pytorch_model.bin:  61%|██████    | 818M/1.34G [00:24<00:15, 33.3MB/s]
Downloading pytorch_model.bin:  62%|██████▏   | 828M/1.34G [00:24<00:15, 34.4MB/s]
Downloading pytorch_model.bin:  62%|██████▏   | 839M/1.34G [00:25<00:14, 35.3MB/s]
Downloading pytorch_model.bin:  63%|██████▎   | 849M/1.34G [00:25<00:13, 36.0MB/s]
Downloading pytorch_model.bin:  64%|██████▍   | 860M/1.34G [00:25<00:14, 33.9MB/s]
Downloading pytorch_model.bin:  65%|██████▍   | 870M/1.34G [00:25<00:13, 34.9MB/s]
Downloading pytorch_model.bin:  65%|██████▌   | 881M/1.34G [00:26<00:12, 35.8MB/s]
Downloading pytorch_model.bin:  66%|██████▋   | 891M/1.34G [00:26<00:12, 36.4MB/s]
Downloading pytorch_model.bin:  67%|██████▋   | 902M/1.34G [00:26<00:12, 36.8MB/s]
Downloading pytorch_model.bin:  68%|██████▊   | 912M/1.34G [00:27<00:11, 37.1MB/s]
Downloading pytorch_model.bin:  69%|██████▊   | 923M/1.34G [00:27<00:11, 37.3MB/s]
Downloading pytorch_model.bin:  69%|██████▉   | 933M/1.34G [00:27<00:11, 37.3MB/s]
Downloading pytorch_model.bin:  70%|███████   | 944M/1.34G [00:27<00:09, 41.1MB/s]
Downloading pytorch_model.bin:  71%|███████   | 954M/1.34G [00:28<00:09, 40.1MB/s]
Downloading pytorch_model.bin:  72%|███████▏  | 965M/1.34G [00:28<00:09, 39.4MB/s]
Downloading pytorch_model.bin:  73%|███████▎  | 975M/1.34G [00:28<00:09, 39.0MB/s]
Downloading pytorch_model.bin:  73%|███████▎  | 986M/1.34G [00:28<00:09, 38.6MB/s]
Downloading pytorch_model.bin:  74%|███████▍  | 996M/1.34G [00:29<00:09, 38.4MB/s]
Downloading pytorch_model.bin:  75%|███████▍  | 1.01G/1.34G [00:29<00:08, 42.0MB/s]
Downloading pytorch_model.bin:  76%|███████▌  | 1.02G/1.34G [00:29<00:08, 40.7MB/s]
Downloading pytorch_model.bin:  76%|███████▋  | 1.03G/1.34G [00:29<00:07, 43.8MB/s]
Downloading pytorch_model.bin:  77%|███████▋  | 1.04G/1.34G [00:30<00:07, 41.7MB/s]
Downloading pytorch_model.bin:  78%|███████▊  | 1.05G/1.34G [00:30<00:07, 40.7MB/s]
Downloading pytorch_model.bin:  79%|███████▊  | 1.06G/1.34G [00:30<00:07, 40.0MB/s]
Downloading pytorch_model.bin:  80%|███████▉  | 1.07G/1.34G [00:30<00:06, 43.2MB/s]
Downloading pytorch_model.bin:  80%|████████  | 1.08G/1.34G [00:31<00:06, 41.6MB/s]
Downloading pytorch_model.bin:  81%|████████  | 1.09G/1.34G [00:31<00:05, 44.2MB/s]
Downloading pytorch_model.bin:  82%|████████▏ | 1.10G/1.34G [00:31<00:05, 42.4MB/s]
Downloading pytorch_model.bin:  83%|████████▎ | 1.11G/1.34G [00:31<00:05, 41.0MB/s]
Downloading pytorch_model.bin:  83%|████████▎ | 1.12G/1.34G [00:32<00:05, 44.2MB/s]
Downloading pytorch_model.bin:  84%|████████▍ | 1.13G/1.34G [00:32<00:05, 42.2MB/s]
Downloading pytorch_model.bin:  85%|████████▍ | 1.14G/1.34G [00:32<00:04, 40.7MB/s]
Downloading pytorch_model.bin:  86%|████████▌ | 1.15G/1.34G [00:32<00:04, 39.8MB/s]
Downloading pytorch_model.bin:  87%|████████▋ | 1.16G/1.34G [00:33<00:04, 42.6MB/s]
Downloading pytorch_model.bin:  87%|████████▋ | 1.17G/1.34G [00:33<00:04, 41.4MB/s]
Downloading pytorch_model.bin:  88%|████████▊ | 1.18G/1.34G [00:33<00:03, 40.4MB/s]
Downloading pytorch_model.bin:  89%|████████▉ | 1.20G/1.34G [00:34<00:03, 39.6MB/s]
Downloading pytorch_model.bin:  90%|████████▉ | 1.21G/1.34G [00:34<00:03, 43.1MB/s]
Downloading pytorch_model.bin:  90%|█████████ | 1.22G/1.34G [00:34<00:03, 41.5MB/s]
Downloading pytorch_model.bin:  91%|█████████ | 1.23G/1.34G [00:34<00:02, 44.7MB/s]
Downloading pytorch_model.bin:  92%|█████████▏| 1.24G/1.34G [00:34<00:02, 42.6MB/s]
Downloading pytorch_model.bin:  93%|█████████▎| 1.25G/1.34G [00:35<00:02, 41.2MB/s]
Downloading pytorch_model.bin:  94%|█████████▎| 1.26G/1.34G [00:35<00:01, 43.8MB/s]
Downloading pytorch_model.bin:  94%|█████████▍| 1.27G/1.34G [00:35<00:01, 42.2MB/s]
Downloading pytorch_model.bin:  95%|█████████▌| 1.28G/1.34G [00:35<00:01, 40.4MB/s]
Downloading pytorch_model.bin:  96%|█████████▌| 1.29G/1.34G [00:36<00:01, 39.9MB/s]
Downloading pytorch_model.bin:  97%|█████████▋| 1.30G/1.34G [00:36<00:01, 39.2MB/s]
Downloading pytorch_model.bin:  97%|█████████▋| 1.31G/1.34G [00:36<00:00, 42.9MB/s]
Downloading pytorch_model.bin:  98%|█████████▊| 1.32G/1.34G [00:36<00:00, 41.1MB/s]
Downloading pytorch_model.bin:  99%|█████████▉| 1.33G/1.34G [00:37<00:00, 40.3MB/s]
Downloading pytorch_model.bin: 100%|█████████▉| 1.34G/1.34G [00:37<00:00, 43.7MB/s]
Downloading pytorch_model.bin: 100%|██████████| 1.34G/1.34G [00:37<00:00, 35.8MB/s]
Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']
- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.534, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.4067470133304596, 'eval_f1': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.0, 'eval_runtime': 3.9436, 'eval_samples_per_second': 20.033, 'eval_steps_per_second': 5.072, 'epoch': 1.0}
{'loss': 0.4203, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.4492512345314026, 'eval_f1': 0.3448275862068966, 'eval_roc_auc': 0.5996835443037974, 'eval_accuracy': 0.25316455696202533, 'eval_runtime': 3.9291, 'eval_samples_per_second': 20.106, 'eval_steps_per_second': 5.09, 'epoch': 2.0}
{'loss': 0.3957, 'learning_rate': 1.902958579881657e-05, 'epoch': 2.98}
{'eval_loss': 0.4018009305000305, 'eval_f1': 0.3709677419354839, 'eval_roc_auc': 0.610759493670886, 'eval_accuracy': 0.2911392405063291, 'eval_runtime': 3.9335, 'eval_samples_per_second': 20.084, 'eval_steps_per_second': 5.085, 'epoch': 3.0}
{'loss': 0.3668, 'learning_rate': 1.8627218934911243e-05, 'epoch': 3.98}
{'eval_loss': 0.42017224431037903, 'eval_f1': 0.32394366197183094, 'eval_roc_auc': 0.5822784810126582, 'eval_accuracy': 0.2911392405063291, 'eval_runtime': 3.9329, 'eval_samples_per_second': 20.087, 'eval_steps_per_second': 5.085, 'epoch': 4.0}
{'loss': 0.3141, 'learning_rate': 1.822485207100592e-05, 'epoch': 4.97}
{'eval_loss': 0.4352617859840393, 'eval_f1': 0.5503355704697986, 'eval_roc_auc': 0.7136075949367089, 'eval_accuracy': 0.5189873417721519, 'eval_runtime': 3.9295, 'eval_samples_per_second': 20.104, 'eval_steps_per_second': 5.09, 'epoch': 5.0}
{'loss': 0.2667, 'learning_rate': 1.7822485207100592e-05, 'epoch': 5.96}
{'eval_loss': 0.4912259578704834, 'eval_f1': 0.425531914893617, 'eval_roc_auc': 0.6392405063291139, 'eval_accuracy': 0.379746835443038, 'eval_runtime': 3.9272, 'eval_samples_per_second': 20.116, 'eval_steps_per_second': 5.093, 'epoch': 6.0}
{'loss': 0.2436, 'learning_rate': 1.7420118343195265e-05, 'epoch': 6.96}
{'eval_loss': 0.4540426433086395, 'eval_f1': 0.5771812080536913, 'eval_roc_auc': 0.7294303797468354, 'eval_accuracy': 0.5189873417721519, 'eval_runtime': 3.9298, 'eval_samples_per_second': 20.103, 'eval_steps_per_second': 5.089, 'epoch': 7.0}
{'loss': 0.1746, 'learning_rate': 1.7017751479289942e-05, 'epoch': 7.95}
{'eval_loss': 0.5415146946907043, 'eval_f1': 0.5, 'eval_roc_auc': 0.685126582278481, 'eval_accuracy': 0.4810126582278481, 'eval_runtime': 3.9297, 'eval_samples_per_second': 20.103, 'eval_steps_per_second': 5.089, 'epoch': 8.0}
{'loss': 0.1285, 'learning_rate': 1.6615384615384618e-05, 'epoch': 8.95}
{'eval_loss': 0.5797773003578186, 'eval_f1': 0.5419354838709678, 'eval_roc_auc': 0.7120253164556961, 'eval_accuracy': 0.5316455696202531, 'eval_runtime': 3.9419, 'eval_samples_per_second': 20.041, 'eval_steps_per_second': 5.074, 'epoch': 9.0}
{'train_runtime': 1091.6135, 'train_samples_per_second': 31.192, 'train_steps_per_second': 7.832, 'train_loss': 0.31512423968299635, 'epoch': 9.0}

Training tner/roberta-large-tweetner7-all:

Downloading (…)okenizer_config.json:   0%|          | 0.00/328 [00:00<?, ?B/s]
Downloading (…)okenizer_config.json: 100%|██████████| 328/328 [00:00<00:00, 199kB/s]

Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.22MB/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.21MB/s]

Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.53MB/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.51MB/s]

Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 14.5MB/s]

Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]
Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 132kB/s]

Downloading (…)lve/main/config.json:   0%|          | 0.00/13.3k [00:00<?, ?B/s]
Downloading (…)lve/main/config.json: 100%|██████████| 13.3k/13.3k [00:00<00:00, 8.35MB/s]

Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]
Downloading pytorch_model.bin:   3%|▎         | 41.9M/1.42G [00:00<00:03, 414MB/s]
Downloading pytorch_model.bin:   7%|▋         | 94.4M/1.42G [00:00<00:02, 444MB/s]
Downloading pytorch_model.bin:  10%|█         | 147M/1.42G [00:00<00:02, 453MB/s] 
Downloading pytorch_model.bin:  14%|█▍        | 199M/1.42G [00:00<00:02, 458MB/s]
Downloading pytorch_model.bin:  18%|█▊        | 252M/1.42G [00:00<00:02, 461MB/s]
Downloading pytorch_model.bin:  21%|██▏       | 304M/1.42G [00:00<00:02, 462MB/s]
Downloading pytorch_model.bin:  25%|██▌       | 357M/1.42G [00:00<00:02, 463MB/s]
Downloading pytorch_model.bin:  29%|██▉       | 409M/1.42G [00:00<00:02, 463MB/s]
Downloading pytorch_model.bin:  33%|███▎      | 461M/1.42G [00:01<00:02, 463MB/s]
Downloading pytorch_model.bin:  36%|███▌      | 514M/1.42G [00:01<00:01, 463MB/s]
Downloading pytorch_model.bin:  40%|███▉      | 566M/1.42G [00:01<00:01, 435MB/s]
Downloading pytorch_model.bin:  44%|████▎     | 619M/1.42G [00:01<00:01, 432MB/s]
Downloading pytorch_model.bin:  47%|████▋     | 671M/1.42G [00:01<00:01, 439MB/s]
Downloading pytorch_model.bin:  51%|█████     | 724M/1.42G [00:01<00:01, 445MB/s]
Downloading pytorch_model.bin:  55%|█████▍    | 776M/1.42G [00:01<00:01, 449MB/s]
Downloading pytorch_model.bin:  58%|█████▊    | 828M/1.42G [00:01<00:01, 451MB/s]
Downloading pytorch_model.bin:  62%|██████▏   | 881M/1.42G [00:01<00:01, 455MB/s]
Downloading pytorch_model.bin:  66%|██████▌   | 933M/1.42G [00:02<00:01, 457MB/s]
Downloading pytorch_model.bin:  70%|██████▉   | 986M/1.42G [00:02<00:00, 459MB/s]
Downloading pytorch_model.bin:  73%|███████▎  | 1.04G/1.42G [00:02<00:00, 458MB/s]
Downloading pytorch_model.bin:  77%|███████▋  | 1.09G/1.42G [00:02<00:00, 458MB/s]
Downloading pytorch_model.bin:  81%|████████  | 1.14G/1.42G [00:02<00:00, 455MB/s]
Downloading pytorch_model.bin:  84%|████████▍ | 1.20G/1.42G [00:02<00:00, 450MB/s]
Downloading pytorch_model.bin:  88%|████████▊ | 1.25G/1.42G [00:02<00:00, 445MB/s]
Downloading pytorch_model.bin:  92%|█████████▏| 1.30G/1.42G [00:02<00:00, 443MB/s]
Downloading pytorch_model.bin:  95%|█████████▌| 1.35G/1.42G [00:03<00:00, 441MB/s]
Downloading pytorch_model.bin:  99%|█████████▉| 1.41G/1.42G [00:03<00:00, 440MB/s]
Downloading pytorch_model.bin: 100%|██████████| 1.42G/1.42G [00:03<00:00, 449MB/s]
Some weights of the model checkpoint at tner/roberta-large-tweetner7-all were not used when initializing RobertaForSequenceClassification: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at tner/roberta-large-tweetner7-all and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.4894, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.3837455213069916, 'eval_f1': 0.4067796610169492, 'eval_roc_auc': 0.6281645569620253, 'eval_accuracy': 0.2911392405063291, 'eval_runtime': 3.6008, 'eval_samples_per_second': 21.939, 'eval_steps_per_second': 5.554, 'epoch': 1.0}
{'loss': 0.4027, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.4371199309825897, 'eval_f1': 0.4657534246575343, 'eval_roc_auc': 0.6629746835443039, 'eval_accuracy': 0.43037974683544306, 'eval_runtime': 3.6072, 'eval_samples_per_second': 21.901, 'eval_steps_per_second': 5.544, 'epoch': 2.0}
{'loss': 0.3505, 'learning_rate': 1.902958579881657e-05, 'epoch': 2.98}
{'eval_loss': 0.4175388514995575, 'eval_f1': 0.4370860927152318, 'eval_roc_auc': 0.6471518987341772, 'eval_accuracy': 0.4050632911392405, 'eval_runtime': 3.6101, 'eval_samples_per_second': 21.883, 'eval_steps_per_second': 5.54, 'epoch': 3.0}
{'loss': 0.2607, 'learning_rate': 1.8627218934911243e-05, 'epoch': 3.98}
{'eval_loss': 0.4827118515968323, 'eval_f1': 0.5194805194805195, 'eval_roc_auc': 0.6977848101265822, 'eval_accuracy': 0.5063291139240507, 'eval_runtime': 3.5985, 'eval_samples_per_second': 21.953, 'eval_steps_per_second': 5.558, 'epoch': 4.0}
{'loss': 0.2002, 'learning_rate': 1.822485207100592e-05, 'epoch': 4.97}
{'eval_loss': 0.6531238555908203, 'eval_f1': 0.5512820512820513, 'eval_roc_auc': 0.7183544303797469, 'eval_accuracy': 0.5316455696202531, 'eval_runtime': 3.6078, 'eval_samples_per_second': 21.897, 'eval_steps_per_second': 5.544, 'epoch': 5.0}
{'loss': 0.1541, 'learning_rate': 1.7822485207100592e-05, 'epoch': 5.96}
{'eval_loss': 0.7045643329620361, 'eval_f1': 0.5569620253164557, 'eval_roc_auc': 0.7231012658227847, 'eval_accuracy': 0.5443037974683544, 'eval_runtime': 3.6046, 'eval_samples_per_second': 21.917, 'eval_steps_per_second': 5.548, 'epoch': 6.0}
{'loss': 0.115, 'learning_rate': 1.7420118343195265e-05, 'epoch': 6.96}
{'eval_loss': 0.7879586219787598, 'eval_f1': 0.5548387096774193, 'eval_roc_auc': 0.7199367088607596, 'eval_accuracy': 0.5443037974683544, 'eval_runtime': 3.6124, 'eval_samples_per_second': 21.869, 'eval_steps_per_second': 5.537, 'epoch': 7.0}
{'loss': 0.0856, 'learning_rate': 1.7017751479289942e-05, 'epoch': 7.95}
{'eval_loss': 0.9052790403366089, 'eval_f1': 0.5064935064935066, 'eval_roc_auc': 0.689873417721519, 'eval_accuracy': 0.4936708860759494, 'eval_runtime': 3.6189, 'eval_samples_per_second': 21.83, 'eval_steps_per_second': 5.527, 'epoch': 8.0}
{'train_runtime': 983.1324, 'train_samples_per_second': 34.634, 'train_steps_per_second': 8.697, 'train_loss': 0.25624070425479734, 'epoch': 8.0}

Training tner/bertweet-large-tweetner7-all:

Downloading (…)okenizer_config.json:   0%|          | 0.00/299 [00:00<?, ?B/s]
Downloading (…)okenizer_config.json: 100%|██████████| 299/299 [00:00<00:00, 197kB/s]

Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.22MB/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.21MB/s]

Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.70MB/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.69MB/s]

Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 5.88MB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 5.85MB/s]

Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]
Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 148kB/s]

Downloading (…)lve/main/config.json:   0%|          | 0.00/13.2k [00:00<?, ?B/s]
Downloading (…)lve/main/config.json: 100%|██████████| 13.2k/13.2k [00:00<00:00, 9.16MB/s]

Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]
Downloading pytorch_model.bin:   1%|          | 10.5M/1.42G [00:00<00:13, 101MB/s]
Downloading pytorch_model.bin:   2%|▏         | 31.5M/1.42G [00:00<00:08, 155MB/s]
Downloading pytorch_model.bin:   4%|▍         | 62.9M/1.42G [00:00<00:06, 224MB/s]
Downloading pytorch_model.bin:   7%|▋         | 105M/1.42G [00:00<00:04, 277MB/s] 
Downloading pytorch_model.bin:  10%|█         | 147M/1.42G [00:00<00:04, 311MB/s]
Downloading pytorch_model.bin:  13%|█▎        | 189M/1.42G [00:00<00:03, 327MB/s]
Downloading pytorch_model.bin:  16%|█▋        | 231M/1.42G [00:00<00:03, 341MB/s]
Downloading pytorch_model.bin:  19%|█▉        | 273M/1.42G [00:00<00:03, 352MB/s]
Downloading pytorch_model.bin:  22%|██▏       | 315M/1.42G [00:01<00:03, 357MB/s]
Downloading pytorch_model.bin:  25%|██▌       | 357M/1.42G [00:01<00:03, 352MB/s]
Downloading pytorch_model.bin:  28%|██▊       | 398M/1.42G [00:01<00:02, 358MB/s]
Downloading pytorch_model.bin:  31%|███       | 440M/1.42G [00:01<00:02, 359MB/s]
Downloading pytorch_model.bin:  34%|███▍      | 482M/1.42G [00:01<00:02, 355MB/s]
Downloading pytorch_model.bin:  37%|███▋      | 524M/1.42G [00:01<00:02, 353MB/s]
Downloading pytorch_model.bin:  40%|███▉      | 566M/1.42G [00:01<00:02, 353MB/s]
Downloading pytorch_model.bin:  43%|████▎     | 608M/1.42G [00:01<00:02, 353MB/s]
Downloading pytorch_model.bin:  46%|████▌     | 650M/1.42G [00:01<00:02, 351MB/s]
Downloading pytorch_model.bin:  49%|████▉     | 692M/1.42G [00:02<00:02, 344MB/s]
Downloading pytorch_model.bin:  52%|█████▏    | 734M/1.42G [00:02<00:02, 312MB/s]
Downloading pytorch_model.bin:  55%|█████▍    | 776M/1.42G [00:02<00:02, 315MB/s]
Downloading pytorch_model.bin:  58%|█████▊    | 818M/1.42G [00:02<00:01, 325MB/s]
Downloading pytorch_model.bin:  61%|██████    | 860M/1.42G [00:02<00:01, 333MB/s]
Downloading pytorch_model.bin:  64%|██████▎   | 902M/1.42G [00:02<00:01, 339MB/s]
Downloading pytorch_model.bin:  67%|██████▋   | 944M/1.42G [00:02<00:01, 344MB/s]
Downloading pytorch_model.bin:  70%|██████▉   | 986M/1.42G [00:02<00:01, 328MB/s]
Downloading pytorch_model.bin:  72%|███████▏  | 1.03G/1.42G [00:03<00:01, 316MB/s]
Downloading pytorch_model.bin:  75%|███████▌  | 1.07G/1.42G [00:03<00:01, 317MB/s]
Downloading pytorch_model.bin:  78%|███████▊  | 1.11G/1.42G [00:03<00:00, 325MB/s]
Downloading pytorch_model.bin:  81%|████████▏ | 1.15G/1.42G [00:03<00:00, 334MB/s]
Downloading pytorch_model.bin:  84%|████████▍ | 1.20G/1.42G [00:03<00:00, 341MB/s]
Downloading pytorch_model.bin:  87%|████████▋ | 1.24G/1.42G [00:03<00:00, 344MB/s]
Downloading pytorch_model.bin:  90%|█████████ | 1.28G/1.42G [00:03<00:00, 348MB/s]
Downloading pytorch_model.bin:  93%|█████████▎| 1.32G/1.42G [00:03<00:00, 350MB/s]
Downloading pytorch_model.bin:  96%|█████████▌| 1.36G/1.42G [00:04<00:00, 352MB/s]
Downloading pytorch_model.bin:  99%|█████████▉| 1.41G/1.42G [00:04<00:00, 353MB/s]
Downloading pytorch_model.bin: 100%|██████████| 1.42G/1.42G [00:04<00:00, 333MB/s]
Some weights of the model checkpoint at tner/bertweet-large-tweetner7-all were not used when initializing RobertaForSequenceClassification: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at tner/bertweet-large-tweetner7-all and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.4826, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.4030444324016571, 'eval_f1': 0.32000000000000006, 'eval_roc_auc': 0.5854430379746836, 'eval_accuracy': 0.25316455696202533, 'eval_runtime': 3.6138, 'eval_samples_per_second': 21.861, 'eval_steps_per_second': 5.534, 'epoch': 1.0}
{'loss': 0.3927, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.36739248037338257, 'eval_f1': 0.5714285714285714, 'eval_roc_auc': 0.7246835443037974, 'eval_accuracy': 0.5063291139240507, 'eval_runtime': 3.6125, 'eval_samples_per_second': 21.869, 'eval_steps_per_second': 5.536, 'epoch': 2.0}
{'loss': 0.3005, 'learning_rate': 1.902958579881657e-05, 'epoch': 2.98}
{'eval_loss': 0.4063909947872162, 'eval_f1': 0.5531914893617021, 'eval_roc_auc': 0.7104430379746836, 'eval_accuracy': 0.4936708860759494, 'eval_runtime': 3.6121, 'eval_samples_per_second': 21.871, 'eval_steps_per_second': 5.537, 'epoch': 3.0}
{'loss': 0.2261, 'learning_rate': 1.8627218934911243e-05, 'epoch': 3.98}
{'eval_loss': 0.4472181499004364, 'eval_f1': 0.6081081081081081, 'eval_roc_auc': 0.7468354430379747, 'eval_accuracy': 0.569620253164557, 'eval_runtime': 3.624, 'eval_samples_per_second': 21.799, 'eval_steps_per_second': 5.519, 'epoch': 4.0}
{'loss': 0.1509, 'learning_rate': 1.822485207100592e-05, 'epoch': 4.97}
{'eval_loss': 0.5047456622123718, 'eval_f1': 0.6064516129032259, 'eval_roc_auc': 0.7515822784810127, 'eval_accuracy': 0.5949367088607594, 'eval_runtime': 3.6105, 'eval_samples_per_second': 21.881, 'eval_steps_per_second': 5.539, 'epoch': 5.0}
{'loss': 0.0955, 'learning_rate': 1.7822485207100592e-05, 'epoch': 5.96}
{'eval_loss': 0.590864360332489, 'eval_f1': 0.6329113924050633, 'eval_roc_auc': 0.7705696202531646, 'eval_accuracy': 0.620253164556962, 'eval_runtime': 3.6172, 'eval_samples_per_second': 21.84, 'eval_steps_per_second': 5.529, 'epoch': 6.0}
{'loss': 0.059, 'learning_rate': 1.7420118343195265e-05, 'epoch': 6.96}
{'eval_loss': 0.6891030669212341, 'eval_f1': 0.6153846153846154, 'eval_roc_auc': 0.7579113924050632, 'eval_accuracy': 0.6075949367088608, 'eval_runtime': 3.612, 'eval_samples_per_second': 21.872, 'eval_steps_per_second': 5.537, 'epoch': 7.0}
{'loss': 0.0499, 'learning_rate': 1.7017751479289942e-05, 'epoch': 7.95}
{'eval_loss': 0.8557901382446289, 'eval_f1': 0.573248407643312, 'eval_roc_auc': 0.7325949367088608, 'eval_accuracy': 0.5569620253164557, 'eval_runtime': 3.6148, 'eval_samples_per_second': 21.854, 'eval_steps_per_second': 5.533, 'epoch': 8.0}
{'train_runtime': 1037.923, 'train_samples_per_second': 32.806, 'train_steps_per_second': 8.238, 'train_loss': 0.2183786487637077, 'epoch': 8.0}

Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Training cardiffnlp/twitter-roberta-large-2022-154m:
{'loss': 0.4905, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.44934946298599243, 'eval_f1': 0.04938271604938272, 'eval_roc_auc': 0.5126582278481012, 'eval_accuracy': 0.02531645569620253, 'eval_runtime': 3.6142, 'eval_samples_per_second': 21.858, 'eval_steps_per_second': 5.534, 'epoch': 1.0}
Traceback (most recent call last):
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 668, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/232: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "script.py", line 66, in <module>
    trainer.train()
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1994, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 2240, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 2331, in _save_checkpoint
    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 442, in save
    return
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 291, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 1630322880 vs 1630322768
Training cardiffnlp/twitter-roberta-large-2022-154m:

Downloading (…)okenizer_config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]
Downloading (…)okenizer_config.json: 100%|██████████| 425/425 [00:00<00:00, 107kB/s]

Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 4.46MB/s]
Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 4.43MB/s]

Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.54MB/s]
Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.53MB/s]

Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 5.85MB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 5.82MB/s]

Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]
Downloading (…)cial_tokens_map.json: 100%|██████████| 280/280 [00:00<00:00, 173kB/s]

Downloading (…)lve/main/config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]
Downloading (…)lve/main/config.json: 100%|██████████| 723/723 [00:00<00:00, 201kB/s]

Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]
Downloading pytorch_model.bin:   1%|▏         | 21.0M/1.42G [00:00<00:10, 136MB/s]
Downloading pytorch_model.bin:   4%|▎         | 52.4M/1.42G [00:00<00:06, 212MB/s]
Downloading pytorch_model.bin:   7%|▋         | 94.4M/1.42G [00:00<00:04, 271MB/s]
Downloading pytorch_model.bin:  10%|▉         | 136M/1.42G [00:00<00:04, 313MB/s] 
Downloading pytorch_model.bin:  13%|█▎        | 178M/1.42G [00:00<00:03, 341MB/s]
Downloading pytorch_model.bin:  15%|█▌        | 220M/1.42G [00:00<00:03, 356MB/s]
Downloading pytorch_model.bin:  18%|█▊        | 262M/1.42G [00:00<00:03, 369MB/s]
Downloading pytorch_model.bin:  21%|██▏       | 304M/1.42G [00:00<00:02, 375MB/s]
Downloading pytorch_model.bin:  24%|██▍       | 346M/1.42G [00:01<00:02, 378MB/s]
Downloading pytorch_model.bin:  27%|██▋       | 388M/1.42G [00:01<00:02, 383MB/s]
Downloading pytorch_model.bin:  30%|███       | 430M/1.42G [00:01<00:02, 386MB/s]
Downloading pytorch_model.bin:  33%|███▎      | 472M/1.42G [00:01<00:02, 389MB/s]
Downloading pytorch_model.bin:  36%|███▌      | 514M/1.42G [00:01<00:02, 392MB/s]
Downloading pytorch_model.bin:  39%|███▉      | 556M/1.42G [00:01<00:02, 391MB/s]
Downloading pytorch_model.bin:  42%|████▏     | 598M/1.42G [00:01<00:02, 362MB/s]
Downloading pytorch_model.bin:  45%|████▍     | 640M/1.42G [00:01<00:02, 347MB/s]
Downloading pytorch_model.bin:  48%|████▊     | 682M/1.42G [00:01<00:02, 356MB/s]
Downloading pytorch_model.bin:  51%|█████     | 724M/1.42G [00:02<00:01, 366MB/s]
Downloading pytorch_model.bin:  54%|█████▍    | 765M/1.42G [00:02<00:01, 373MB/s]
Downloading pytorch_model.bin:  57%|█████▋    | 807M/1.42G [00:02<00:01, 379MB/s]
Downloading pytorch_model.bin:  60%|█████▉    | 849M/1.42G [00:02<00:01, 384MB/s]
Downloading pytorch_model.bin:  63%|██████▎   | 891M/1.42G [00:02<00:01, 386MB/s]
Downloading pytorch_model.bin:  66%|██████▌   | 933M/1.42G [00:02<00:01, 389MB/s]
Downloading pytorch_model.bin:  69%|██████▊   | 975M/1.42G [00:02<00:01, 392MB/s]
Downloading pytorch_model.bin:  72%|███████▏  | 1.02G/1.42G [00:02<00:01, 394MB/s]
Downloading pytorch_model.bin:  74%|███████▍  | 1.06G/1.42G [00:02<00:00, 395MB/s]
Downloading pytorch_model.bin:  77%|███████▋  | 1.10G/1.42G [00:03<00:00, 393MB/s]
Downloading pytorch_model.bin:  80%|████████  | 1.14G/1.42G [00:03<00:00, 392MB/s]
Downloading pytorch_model.bin:  83%|████████▎ | 1.18G/1.42G [00:03<00:00, 391MB/s]
Downloading pytorch_model.bin:  86%|████████▋ | 1.23G/1.42G [00:03<00:00, 387MB/s]
Downloading pytorch_model.bin:  89%|████████▉ | 1.27G/1.42G [00:03<00:00, 381MB/s]
Downloading pytorch_model.bin:  92%|█████████▏| 1.31G/1.42G [00:03<00:00, 364MB/s]
Downloading pytorch_model.bin:  95%|█████████▌| 1.35G/1.42G [00:03<00:00, 343MB/s]
Downloading pytorch_model.bin:  98%|█████████▊| 1.39G/1.42G [00:03<00:00, 337MB/s]
Downloading pytorch_model.bin: 100%|██████████| 1.42G/1.42G [00:03<00:00, 362MB/s]
Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.4908, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.43706512451171875, 'eval_f1': 0.21782178217821782, 'eval_roc_auc': 0.5522151898734178, 'eval_accuracy': 0.13924050632911392, 'eval_runtime': 3.5958, 'eval_samples_per_second': 21.97, 'eval_steps_per_second': 5.562, 'epoch': 1.0}
Traceback (most recent call last):
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 668, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/232: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "script.py", line 66, in <module>
    trainer.train()
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1994, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 2240, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 2331, in _save_checkpoint
    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 442, in save
    return
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 291, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 1630322880 vs 1630322768
Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Training cardiffnlp/twitter-roberta-large-2022-154m:
{'loss': 0.5114, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.42601168155670166, 'eval_f1': 0.18749999999999997, 'eval_roc_auc': 0.5443037974683544, 'eval_accuracy': 0.11392405063291139, 'eval_runtime': 3.5923, 'eval_samples_per_second': 21.991, 'eval_steps_per_second': 5.567, 'epoch': 1.0}
{'loss': 0.3858, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.39668139815330505, 'eval_f1': 0.5, 'eval_roc_auc': 0.6819620253164557, 'eval_accuracy': 0.43037974683544306, 'eval_runtime': 3.6269, 'eval_samples_per_second': 21.782, 'eval_steps_per_second': 5.514, 'epoch': 2.0}
Traceback (most recent call last):
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 668, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/233: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "script.py", line 66, in <module>
    trainer.train()
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1994, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 2240, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 2331, in _save_checkpoint
    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 442, in save
    return
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/torch/serialization.py", line 291, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 1647100672 vs 1647100560
Traceback (most recent call last):
  File "script.py", line 1, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Training cardiffnlp/twitter-roberta-large-2022-154m:
{'loss': 0.5075, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.3736010789871216, 'eval_f1': 0.5277777777777778, 'eval_roc_auc': 0.6977848101265822, 'eval_accuracy': 0.27848101265822783, 'eval_runtime': 3.7222, 'eval_samples_per_second': 21.224, 'eval_steps_per_second': 5.373, 'epoch': 1.0}
{'loss': 0.3671, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.40346941351890564, 'eval_f1': 0.44776119402985076, 'eval_roc_auc': 0.6503164556962026, 'eval_accuracy': 0.379746835443038, 'eval_runtime': 3.6211, 'eval_samples_per_second': 21.816, 'eval_steps_per_second': 5.523, 'epoch': 2.0}
{'loss': 0.2754, 'learning_rate': 1.902958579881657e-05, 'epoch': 2.98}
{'eval_loss': 0.448822021484375, 'eval_f1': 0.5359477124183006, 'eval_roc_auc': 0.7072784810126583, 'eval_accuracy': 0.5063291139240507, 'eval_runtime': 3.6213, 'eval_samples_per_second': 21.816, 'eval_steps_per_second': 5.523, 'epoch': 3.0}
{'loss': 0.1914, 'learning_rate': 1.8627218934911243e-05, 'epoch': 3.98}
{'eval_loss': 0.5513738989830017, 'eval_f1': 0.5548387096774193, 'eval_roc_auc': 0.7199367088607596, 'eval_accuracy': 0.5443037974683544, 'eval_runtime': 3.6263, 'eval_samples_per_second': 21.785, 'eval_steps_per_second': 5.515, 'epoch': 4.0}
{'loss': 0.1107, 'learning_rate': 1.822485207100592e-05, 'epoch': 4.97}
{'eval_loss': 0.8209577202796936, 'eval_f1': 0.5316455696202531, 'eval_roc_auc': 0.7072784810126582, 'eval_accuracy': 0.5316455696202531, 'eval_runtime': 3.6288, 'eval_samples_per_second': 21.77, 'eval_steps_per_second': 5.511, 'epoch': 5.0}
{'loss': 0.0745, 'learning_rate': 1.7822485207100592e-05, 'epoch': 5.96}
{'eval_loss': 0.7622570395469666, 'eval_f1': 0.5822784810126582, 'eval_roc_auc': 0.7389240506329114, 'eval_accuracy': 0.5822784810126582, 'eval_runtime': 3.6226, 'eval_samples_per_second': 21.808, 'eval_steps_per_second': 5.521, 'epoch': 6.0}
Traceback (most recent call last):
  File "script.py", line 66, in <module>
    trainer.train()
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/trainer.py", line 1904, in _inner_training_loop
    if (
KeyboardInterrupt
Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/NLP/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Training cardiffnlp/twitter-roberta-large-2022-154m:
{'loss': 0.5017, 'learning_rate': 1.983431952662722e-05, 'epoch': 0.99}
{'eval_loss': 0.4318236708641052, 'eval_f1': 0.33027522935779813, 'eval_roc_auc': 0.5949367088607594, 'eval_accuracy': 0.22784810126582278, 'eval_runtime': 3.8505, 'eval_samples_per_second': 20.517, 'eval_steps_per_second': 5.194, 'epoch': 1.0}
{'loss': 0.385, 'learning_rate': 1.9431952662721897e-05, 'epoch': 1.99}
{'eval_loss': 0.39910298585891724, 'eval_f1': 0.48, 'eval_roc_auc': 0.6724683544303797, 'eval_accuracy': 0.45569620253164556, 'eval_runtime': 3.6276, 'eval_samples_per_second': 21.778, 'eval_steps_per_second': 5.513, 'epoch': 2.0}
{'loss': 0.2878, 'learning_rate': 1.902958579881657e-05, 'epoch': 2.98}
{'eval_loss': 0.5036174654960632, 'eval_f1': 0.47368421052631576, 'eval_roc_auc': 0.6693037974683544, 'eval_accuracy': 0.45569620253164556, 'eval_runtime': 3.6161, 'eval_samples_per_second': 21.847, 'eval_steps_per_second': 5.531, 'epoch': 3.0}
{'loss': 0.2057, 'learning_rate': 1.8627218934911243e-05, 'epoch': 3.98}
{'eval_loss': 0.4594968557357788, 'eval_f1': 0.6447368421052632, 'eval_roc_auc': 0.7721518987341772, 'eval_accuracy': 0.620253164556962, 'eval_runtime': 3.6193, 'eval_samples_per_second': 21.827, 'eval_steps_per_second': 5.526, 'epoch': 4.0}
{'loss': 0.1303, 'learning_rate': 1.822485207100592e-05, 'epoch': 4.97}
{'eval_loss': 0.7859923839569092, 'eval_f1': 0.5032258064516129, 'eval_roc_auc': 0.6882911392405063, 'eval_accuracy': 0.4936708860759494, 'eval_runtime': 3.6123, 'eval_samples_per_second': 21.87, 'eval_steps_per_second': 5.537, 'epoch': 5.0}
{'loss': 0.0979, 'learning_rate': 1.7822485207100592e-05, 'epoch': 5.96}
{'eval_loss': 0.6715871095657349, 'eval_f1': 0.5935483870967743, 'eval_roc_auc': 0.7436708860759493, 'eval_accuracy': 0.5822784810126582, 'eval_runtime': 3.6155, 'eval_samples_per_second': 21.85, 'eval_steps_per_second': 5.532, 'epoch': 6.0}
{'train_runtime': 746.3872, 'train_samples_per_second': 45.62, 'train_steps_per_second': 11.455, 'train_loss': 0.26846077892980147, 'epoch': 6.0}

