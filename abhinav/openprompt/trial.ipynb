{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/abhinavkumar2/nlp2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/home/abhinavkumar2/nlp2/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "from openprompt import PromptDataLoader\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
    "    \"no_influencer\",\n",
    "    \"nano\",\n",
    "    \"micro\",\n",
    "    \"macro\",\n",
    "    \"mega\"\n",
    "]\n",
    "dataset = [ # For simplicity, there's only two examples\n",
    "    # text_a is the input text of the data, some other datasets may have multiple input sentences in one example.\n",
    "    InputExample(\n",
    "        guid = 0,\n",
    "        text_a = \"Albert Einstein was one of the greatest intellects of his time.\",\n",
    "    ),\n",
    "    InputExample(\n",
    "        guid = 1,\n",
    "        text_a = \"The film was badly made.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/mnt/home/abhinavkumar2/Profiling-Cryptocurrency-Influencers-with-FSL/data/few_shot_train_val_test/test.csv')\n",
    "test_df = test_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/mnt/home/abhinavkumar2/Profiling-Cryptocurrency-Influencers-with-FSL/data/few_shot_train_val_test/train.csv')\n",
    "train_df = train_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df = pd.read_csv('/mnt/home/abhinavkumar2/Profiling-Cryptocurrency-Influencers-with-FSL/data/few_shot_train_val_test/validate.csv')\n",
    "validate_df = validate_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, validate_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetProcessor(DataProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.labels = ['no_influencer', 'nano', 'micro', 'macro', 'mega']\n",
    "\n",
    "    def get_examples(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        labels = df['class'].unique().tolist()\n",
    "        id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "        label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "        df = df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            examples.append(InputExample(guid=row['twitter user id'], text_a=row['texts'], label= label2id[row['class']]))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['train'] =  TweetProcessor().get_examples('/mnt/home/abhinavkumar2/Profiling-Cryptocurrency-Influencers-with-FSL/data/few_shot_train_val_test/train.csv')\n",
    "dataset['test'] = TweetProcessor().get_examples('/mnt/home/abhinavkumar2/Profiling-Cryptocurrency-Influencers-with-FSL/data/few_shot_train_val_test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"cardiffnlp/twitter-roberta-large-2022-154m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "promptTemplate = ManualTemplate(tokenizer = tokenizer).from_file(\"./mt.txt\", choice=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes= [\"no_influencer\", \"nano\", \"micro\", \"macro\", \"mega\"],\n",
    "    label_words = {\n",
    "        \"no_influencer\": [\"zero\", \"none\", \"nothing\"],\n",
    "        \"nano\": [\"smallest\", \"least\"],\n",
    "        \"micro\": [\"medium\", \"small\", \"few\"],\n",
    "        \"macro\": [\"large\", \"big\", \"many\"],\n",
    "        \"mega\": [\"largest\", \"biggest\", \"most\"],\n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "support_sampler = FewShotSampler(num_examples_total=100, also_sample_dev=False)\n",
    "dataset['support'] = support_sampler(dataset['train'], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100it [00:00, 284.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for example in dataset['support']:\n",
    "    example.label = -1 # remove the labels of support set for classification\n",
    "support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=promptTemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
    "    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=promptTemplate, verbalizer=promptVerbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ContextCali: 100%|██████████| 20/20 [00:09<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the calibration logits is tensor([45.1223, -5.0615, 46.4884,  ...,  0.4226,  1.7090, 32.2724],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(4)]\n",
    "from openprompt.utils.calibrate import calibrate\n",
    "# calculate the calibration logits\n",
    "cc_logits = calibrate(prompt_model, support_dataloader)\n",
    "print(\"the calibration logits is\", cc_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of label words per class: [3, 2, 3, 3] \n",
      " After filtering, number of label words per class: [3, 2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
    "# currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
    "prompt_model.verbalizer.register_calibrate_logits(cc_logits)\n",
    "new_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(4)]\n",
    "print(\"Original number of label words per class: {} \\n After filtering, number of label words per class: {}\".format(org_label_words_num, new_label_words_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 15it [00:00, 374.15it/s]\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 0 tensor([0, 1, 2, 3, 4], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:00<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 1 tensor([4, 1, 2, 0, 3], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 2 tensor([3, 2, 1, 0, 4], device='cuda:0')\n",
      "test: 0.13333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# zero-shot test\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=promptTemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
    "    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "pbar = tqdm(test_dataloader)\n",
    "for step, inputs in enumerate(pbar):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    print('steps', step, labels)\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"test:\", acc)  # roughly ~0.853 when using template 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
