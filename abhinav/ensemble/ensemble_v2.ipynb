{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, ElectraTokenizer, ElectraModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = \"cardiffnlp/twitter-roberta-large-2022-154m\"\n",
    "model_name_2 = 'Twitter/twhin-bert-base'\n",
    "\n",
    "# model_name_1 = \"cardiffnlp/twitter-roberta-large-2022-154m\"\n",
    "# model_name_2 = 'ElKulako/cryptobert'\n",
    "\n",
    "# model_name_1 = \"cardiffnlp/twitter-roberta-large-2022-154m\"\n",
    "# model_name_2 = 'svalabs/twitter-xlm-roberta-crypto-spam'\n",
    "\n",
    "# model_name_1 = \"ElKulako/cryptobert\"\n",
    "# model_name_2 = 'svalabs/twitter-xlm-roberta-crypto-spam'\n",
    "\n",
    "\n",
    "\n",
    "n1 = 0 # number of layers to freeze\n",
    "n2 = 0 # number of layers to freeze\n",
    "EPOCHS = 20\n",
    "\n",
    "# batch size = mini_batch_size * accumulation_steps\n",
    "mini_batch_size = 1\n",
    "accumulation_steps = 16\n",
    "\n",
    "max_tweet_len = 512\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBodyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model_1, model_2, num_labels):\n",
    "        super(TwoBodyModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.model_1 = AutoModel.from_pretrained(model_1)\n",
    "        self.model_2 = AutoModel.from_pretrained(model_2)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.pre_classifier_1 = nn.Linear(self.model_1.config.hidden_size, self.model_1.config.hidden_size)\n",
    "        self.pre_classifier_2 = nn.Linear(self.model_2.config.hidden_size, self.model_2.config.hidden_size)\n",
    "        self.dropout_1_2 = nn.Dropout(0.1)\n",
    "        self.dropout_2_2 = nn.Dropout(0.1)\n",
    "        self.classifier_1 = nn.Linear(self.model_1.config.hidden_size+self.model_2.config.hidden_size, self.model_1.config.hidden_size+self.model_2.config.hidden_size)\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "        self.classifier_2 = nn.Linear(self.model_1.config.hidden_size+self.model_2.config.hidden_size, num_labels)\n",
    "\n",
    "    # def freeze_layers(self, model, num_layers):\n",
    "    #     # Freeze the first 'num_layers' layers\n",
    "    #     for i, layer in enumerate(model.encoder.layer):\n",
    "    #         if i < num_layers:\n",
    "    #             for param in layer.parameters():\n",
    "    #                 param.requires_grad = False\n",
    "    def freeze_all_layers(self):\n",
    "        # Freeze the first 'num_layers' layers\n",
    "        for i, layer in enumerate(self.model_1.encoder.layer):\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        for i, layer in enumerate(self.model_2.encoder.layer):\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "                    \n",
    "    def unfreeze_all_layers(self):\n",
    "        # Freeze the first 'num_layers' layers\n",
    "        for i, layer in enumerate(self.model_1.encoder.layer):\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        for i, layer in enumerate(self.model_2.encoder.layer):\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def count_parameters(self):\n",
    "            total_params = sum(p.numel() for p in self.parameters())\n",
    "            total_trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "            print(f'The model has {total_params:,} total parameters, and {total_trainable_params:,} trainable parameters')\n",
    "    \n",
    "    def forward(self, input_ids_1, input_ids_2, attention_mask_1, attention_mask_2, labels):\n",
    "\n",
    "        output_1 = self.model_1(input_ids=input_ids_1, attention_mask=attention_mask_1)\n",
    "        output_2 = self.model_2(input_ids=input_ids_2, attention_mask=attention_mask_2)\n",
    "\n",
    "        pre_output_1 = self.dropout_1(F.gelu(output_1[0][:, 0, :]))\n",
    "        pre_output_2 = self.dropout_2(F.gelu(output_2[0][:, 0, :]))\n",
    "\n",
    "        pre_output_1 = self.pre_classifier_1(pre_output_1)\n",
    "        pre_output_1 = self.dropout_1_2(F.gelu(pre_output_1))\n",
    "        pre_output_2 = self.pre_classifier_2(pre_output_2)\n",
    "        pre_output_2 = self.dropout_2_2(F.gelu(pre_output_2))\n",
    "        output = self.classifier_1(torch.cat((pre_output_1, pre_output_2), 1))\n",
    "\n",
    "        output = self.dropout_3(F.gelu(output))\n",
    "        logits = self.classifier_2(output)\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = AutoTokenizer.from_pretrained(model_name_1)\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/ubuntu/Profiling-Cryptocurrency-Influencers-with-FSL/data/finetune_train_val_test/train.csv')\n",
    "validate_df = pd.read_csv('/home/ubuntu/Profiling-Cryptocurrency-Influencers-with-FSL/data/finetune_train_val_test/validate.csv')\n",
    "train_df = train_df.groupby('twitter user id').agg({'texts': tokenizer_2.sep_token.join, 'class': 'first', 'count_mention': sum}).reset_index()\n",
    "validate_df = validate_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['nano', 'no influencer', 'macro', 'mega', 'micro'],\n",
       " {'nano': 0, 'no influencer': 1, 'macro': 2, 'mega': 3, 'micro': 4})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_df['class'].unique().tolist()\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, max_len, tweet_df):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tweets_dataset = tweet_df\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.tweets_dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets_dataset.iloc[idx]['texts']\n",
    "        label = self.tweets_dataset.iloc[idx]['class']\n",
    "        user_id = self.tweets_dataset.iloc[idx]['twitter user id']\n",
    "        label = label2id[label]\n",
    "        labels_matrix = np.zeros(5)\n",
    "        labels_matrix[label] = 1\n",
    "   \n",
    "        encoding = self.tokenizer(\n",
    "            text = tweet,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'tweet': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(labels_matrix, dtype=torch.float),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'user_id': user_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = TweetDataset(tokenizer_1, max_tweet_len, train_df)\n",
    "val_dataset_1 = TweetDataset(tokenizer_1, max_tweet_len, validate_df)\n",
    "train_dataset_2 = TweetDataset(tokenizer_2, max_tweet_len, train_df)\n",
    "val_dataset_2 = TweetDataset(tokenizer_2, max_tweet_len, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_1 = DataLoader(train_dataset_1, batch_size=mini_batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader_1 = DataLoader(val_dataset_1, batch_size=mini_batch_size, shuffle=True, num_workers=4)\n",
    "train_data_loader_2 = DataLoader(train_dataset_2, batch_size=mini_batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader_2 = DataLoader(val_dataset_2, batch_size=mini_batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at Twitter/twhin-bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Twitter/twhin-bert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TwoBodyModel(model_name_1, model_name_2, 5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader_1, data_loader_2, optimizer, device, scheduler, accumulation_steps): \n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    total_acc = 0\n",
    "    total_counter = 0\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    strt = time.time()\n",
    "    \n",
    "    for i, d in enumerate(zip(data_loader_1, data_loader_2)):\n",
    "        d_1, d_2 = d[0], d[1]\n",
    "\n",
    "        input_ids_1 = d_1[\"input_ids\"].reshape(d_1[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "        attention_mask_1 = d_1[\"attention_mask\"].to(device)\n",
    "        targets_1 = d_1[\"label\"].to(device)\n",
    "\n",
    "        input_ids_2 = d_2[\"input_ids\"].reshape(d_2[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "        attention_mask_2 = d_2[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids_1=input_ids_1, input_ids_2=input_ids_2, attention_mask_1=attention_mask_1,\n",
    "                        attention_mask_2=attention_mask_2, labels=targets_1)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        _, prediction = torch.max(outputs[1], dim=1)\n",
    "        targets = torch.argmax(targets_1, dim=1)\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        accuracy = accuracy_score(targets, prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        total_acc += accuracy\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Accumulate gradients\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization step and zero gradients if it's the last accumulation step\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scheduler.step()\n",
    "\n",
    "        counter += 1\n",
    "        total_counter += 1\n",
    "\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print('iteration ', i, '; acc: ', acc / counter)\n",
    "            warnings.warn('iteration ' + str(i) + '; acc: ' + str(acc / counter) + '; time: ' + str(time.time() - strt) + ' s')\n",
    "            acc = 0\n",
    "            counter = 0\n",
    "            print('time: ', time.time() - strt, ' s')\n",
    "            strt = time.time()\n",
    "\n",
    "    return total_acc / total_counter, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader_1, data_loader_2, device):  #, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    preds = []\n",
    "    targets_array = []\n",
    "    logit_array = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(zip(data_loader_1, data_loader_2)):\n",
    "            d_1, d_2 = d[0], d[1]\n",
    "            input_ids_1 = d_1[\"input_ids\"].reshape(d_1[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "            attention_mask_1 = d_1[\"attention_mask\"].to(device)\n",
    "            targets_1 = d_1[\"label\"].to(device)\n",
    "\n",
    "            input_ids_2 = d_2[\"input_ids\"].reshape(d_2[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "            attention_mask_2 = d_2[\"attention_mask\"].to(device)\n",
    "            targets_2 = d_2[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids_1=input_ids_1, input_ids_2=input_ids_2, attention_mask_1=attention_mask_1, attention_mask_2=attention_mask_2, labels=targets_1)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            _, prediction = torch.max(outputs[1], dim=1)\n",
    "            targets = torch.argmax(targets_1, dim=1)\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            prediction = prediction.cpu().detach().numpy()\n",
    "            accuracy = accuracy_score(targets, prediction)\n",
    "            preds += list(prediction.flatten())\n",
    "            targets_array += list(targets.flatten())\n",
    "\n",
    "            acc += accuracy\n",
    "            losses.append(loss.item())\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "            logit_array += list(logits.cpu().detach().numpy())\n",
    "\n",
    "    return acc / counter, np.mean(losses), preds, np.array(logit_array), targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.02},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.01}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_data_loader_1) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 639,050,501 total parameters, and 250,900,997 trainable parameters\n",
      "\n",
      "Epoch 1/20\n",
      "----------\n",
      "Train loss 1.609300583847298 Train accuracy 0.18181818181818182\n",
      "Val loss 1.6079650402069092 Val accuracy 0.2\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "Train loss 1.6082554040861523 Train accuracy 0.2396694214876033\n",
      "Val loss 1.6110499143600463 Val accuracy 0.2\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "Train loss 1.6076200766996904 Train accuracy 0.19834710743801653\n",
      "Val loss 1.6041122833887735 Val accuracy 0.2\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "Train loss 1.6076947401377781 Train accuracy 0.23140495867768596\n",
      "Val loss 1.6062918583552042 Val accuracy 0.2\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "The model has 639,050,501 total parameters, and 639,050,501 trainable parameters\n",
      "Train loss 1.6070984661086531 Train accuracy 0.256198347107438\n",
      "Val loss 1.6079113483428955 Val accuracy 0.2\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "Train loss 1.5935002732868038 Train accuracy 0.23140495867768596\n",
      "Val loss 1.592115863164266 Val accuracy 0.2\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "Train loss 1.5830756425857544 Train accuracy 0.21487603305785125\n",
      "Val loss 1.5788801113764446 Val accuracy 0.13333333333333333\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "Train loss 1.5101801137293667 Train accuracy 0.4132231404958678\n",
      "Val loss 1.4385303179423015 Val accuracy 0.4666666666666667\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "Train loss 1.324587015081043 Train accuracy 0.5206611570247934\n",
      "Val loss 1.3874425172805787 Val accuracy 0.3333333333333333\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "Train loss 1.207351742697156 Train accuracy 0.5537190082644629\n",
      "Val loss 1.245282522837321 Val accuracy 0.4666666666666667\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "Train loss 1.0595476558385801 Train accuracy 0.6446280991735537\n",
      "Val loss 1.4362509330113729 Val accuracy 0.3333333333333333\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "Train loss 0.8422635086804382 Train accuracy 0.7851239669421488\n",
      "Val loss 1.235242740313212 Val accuracy 0.4\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "Train loss 0.6543092642687569 Train accuracy 0.7851239669421488\n",
      "Val loss 1.1852888494729996 Val accuracy 0.3333333333333333\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "Train loss 0.4792321945024916 Train accuracy 0.8677685950413223\n",
      "Val loss 1.4765862360596658 Val accuracy 0.4666666666666667\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "Train loss 0.36529208516532724 Train accuracy 0.9421487603305785\n",
      "Val loss 1.3930334068834782 Val accuracy 0.4666666666666667\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "Train loss 0.3175144981716044 Train accuracy 0.9256198347107438\n",
      "Val loss 1.797508279979229 Val accuracy 0.4666666666666667\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "Train loss 0.22133203595876694 Train accuracy 0.9669421487603306\n",
      "Val loss 1.9838885003079971 Val accuracy 0.5333333333333333\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "Train loss 0.3766563170212359 Train accuracy 0.9090909090909091\n",
      "Val loss 2.0978292798623444 Val accuracy 0.3333333333333333\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "Train loss 0.3904078571387559 Train accuracy 0.8677685950413223\n",
      "Val loss 1.9656037181615829 Val accuracy 0.4666666666666667\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "Train loss 0.24176003593828313 Train accuracy 0.9173553719008265\n",
      "Val loss 1.180174009501934 Val accuracy 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       1.00      0.33      0.50         3\n",
      "           2       1.00      0.33      0.50         3\n",
      "           3       0.60      1.00      0.75         3\n",
      "           4       0.60      1.00      0.75         3\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.77      0.67      0.63        15\n",
      "weighted avg       0.77      0.67      0.63        15\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "real_preds = []\n",
    "tt = None\n",
    "pp = None\n",
    "vacc = 0\n",
    "# Freeze all encoder layers\n",
    "model.freeze_all_layers()\n",
    "model.count_parameters()\n",
    "for epoch in range(EPOCHS):\n",
    "    print()\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Unfreeze all encoders layers in epoch 4\n",
    "    if (epoch+1 == 5):\n",
    "        model.unfreeze_all_layers()\n",
    "        model.count_parameters()\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader_1,\n",
    "        train_data_loader_2,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        accumulation_steps\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss, predictions, val_logits, targets_arr = eval_model(\n",
    "        model,\n",
    "        val_data_loader_1,\n",
    "        val_data_loader_2,\n",
    "        device\n",
    "    )\n",
    "    if val_acc > vacc:\n",
    "        tt = targets_arr\n",
    "        pp = predictions\n",
    "        vacc = val_acc\n",
    "\n",
    "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
    "\n",
    "    if epoch+1 > 12:\n",
    "        # Save models after every epoch larger than 14 to avoid overfitting\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "\n",
    "cm = confusion_matrix(tt, pp)\n",
    "print(classification_report(tt, pp))\n",
    "\n",
    "# Create a heatmap plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
