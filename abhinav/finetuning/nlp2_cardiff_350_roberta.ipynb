{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch-summary"
      ],
      "metadata": {
        "id": "pSeObdqhAfue"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c_fOlAwqAN5m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPL5B1q3AN5p"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lZbcko2VAN5q"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"pig4431/TweetEval_roBERTa_5E\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-large-2022-154m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "InNaYDAIAN5q"
      },
      "outputs": [],
      "source": [
        "# read dataframe from folder data and save it to variable df\n",
        "# test_df = pd.read_csv('/mnt/home/abhinavkumar2/Profiling-Cryptocurrency-Influencers-with-FSL/data/test.csv')\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "validate_df = pd.read_csv('/content/validate.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NDsLafHKAN5q"
      },
      "outputs": [],
      "source": [
        "# group the df by twitter user id and aggregate the texts and keep other columns as it is\n",
        "train_df = train_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IKXfS86oAN5r"
      },
      "outputs": [],
      "source": [
        "validate_df = validate_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0ssQp-uAN5r",
        "outputId": "1727662a-e3e0-47be-8d98-5d528942e119"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['nano', 'no influencer', 'macro', 'mega', 'micro'],\n",
              " {'nano': 0, 'no influencer': 1, 'macro': 2, 'mega': 3, 'micro': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "labels = train_df['class'].unique().tolist()\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "labels, label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5SiftNVFAN5r"
      },
      "outputs": [],
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, max_len, tweet_df):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.tweets_dataset = tweet_df\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.tweets_dataset)\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets_dataset.iloc[idx]['texts']\n",
        "        label = self.tweets_dataset.iloc[idx]['class']\n",
        "        user_id = self.tweets_dataset.iloc[idx]['twitter user id']\n",
        "        label = label2id[label]\n",
        "        labels_matrix = np.zeros(5)\n",
        "        labels_matrix[label] = 1\n",
        "   \n",
        "        encoding = self.tokenizer(\n",
        "            text = tweet,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'tweet': tweet,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(labels_matrix, dtype=torch.float),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'user_id': user_id\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3un5i5V5AN5s"
      },
      "outputs": [],
      "source": [
        "train_dataset = TweetDataset(tokenizer, 256, train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUqkvpdbAN5s"
      },
      "outputs": [],
      "source": [
        "train_encoded = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdP12WuJAN5s",
        "outputId": "f743fcf5-f7dc-42f2-9efd-761c79544436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "val_dataset = TweetDataset(tokenizer, 256, validate_df)\n",
        "val_encoded = DataLoader(val_dataset, batch_size=10, shuffle=True, num_workers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puGCII4fAN5t",
        "outputId": "30ca3f61-0c06-43b1-f076-c04c6555225e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-large-2022-154m\", \n",
        "                                                           problem_type=\"multi_label_classification\", \n",
        "                                                           num_labels=len(labels),\n",
        "                                                           ignore_mismatched_sizes=True)\n",
        "# model = RobertaForSequenceClassification.from_pretrained( \"pig4431/TweetEval_roBERTa_5E\",\n",
        "#                                                          num_labels=len(labels),\n",
        "#                                                          problem_type=\"multi_label_classification\",\n",
        "#                                                          ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neN4Mn_xGx0V",
        "outputId": "6bcc3f70-8023-4865-8d5b-6e6c6e0cb790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B-YCYx0AN5t",
        "outputId": "8ec43c89-735f-4aa7-c69e-7dbbadaf11d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─RobertaModel: 1-1                      --\n",
            "|    └─RobertaEmbeddings: 2-1            --\n",
            "|    |    └─Embedding: 3-1               51,471,360\n",
            "|    |    └─Embedding: 3-2               526,336\n",
            "|    |    └─Embedding: 3-3               1,024\n",
            "|    |    └─LayerNorm: 3-4               2,048\n",
            "|    |    └─Dropout: 3-5                 --\n",
            "|    └─RobertaEncoder: 2-2               --\n",
            "|    |    └─ModuleList: 3-6              302,309,376\n",
            "├─RobertaClassificationHead: 1-2         --\n",
            "|    └─Linear: 2-3                       1,049,600\n",
            "|    └─Dropout: 2-4                      --\n",
            "|    └─Linear: 2-5                       5,125\n",
            "=================================================================\n",
            "Total params: 355,364,869\n",
            "Trainable params: 355,364,869\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "├─RobertaModel: 1-1                      --\n",
              "|    └─RobertaEmbeddings: 2-1            --\n",
              "|    |    └─Embedding: 3-1               51,471,360\n",
              "|    |    └─Embedding: 3-2               526,336\n",
              "|    |    └─Embedding: 3-3               1,024\n",
              "|    |    └─LayerNorm: 3-4               2,048\n",
              "|    |    └─Dropout: 3-5                 --\n",
              "|    └─RobertaEncoder: 2-2               --\n",
              "|    |    └─ModuleList: 3-6              302,309,376\n",
              "├─RobertaClassificationHead: 1-2         --\n",
              "|    └─Linear: 2-3                       1,049,600\n",
              "|    └─Dropout: 2-4                      --\n",
              "|    └─Linear: 2-5                       5,125\n",
              "=================================================================\n",
              "Total params: 355,364,869\n",
              "Trainable params: 355,364,869\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mYnijuc-AN5u"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8aSPEoJFAN5u"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"roberta-tweet-english\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    save_total_limit = 3,\n",
        "    warmup_steps=100,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    #push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZzZ53eDKAN5u"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "    \n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    # return as dictionary\n",
        "    metrics = {'f1': f1_micro_average,\n",
        "               'roc_auc': roc_auc,\n",
        "               'accuracy': accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, \n",
        "            tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds, \n",
        "        labels=p.label_ids)\n",
        "    print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LWdfhs1eAN5w"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x2ZERNtEAN5x",
        "outputId": "9d13c5ac-a1d9-4675-d885-ef077dc2dc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [650/650 29:03, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Roc Auc</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.656600</td>\n",
              "      <td>0.649258</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>0.559987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.543700</td>\n",
              "      <td>0.485227</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.490200</td>\n",
              "      <td>0.490766</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.485400</td>\n",
              "      <td>0.482454</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.452800</td>\n",
              "      <td>0.462518</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.344100</td>\n",
              "      <td>0.450201</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.298700</td>\n",
              "      <td>0.498754</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.289800</td>\n",
              "      <td>0.444780</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.172600</td>\n",
              "      <td>0.549711</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.150500</td>\n",
              "      <td>0.485782</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.683333</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.097900</td>\n",
              "      <td>0.507612</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.107200</td>\n",
              "      <td>0.591735</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.683333</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.054400</td>\n",
              "      <td>0.625190</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.683333</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.072300</td>\n",
              "      <td>0.642113</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.024900</td>\n",
              "      <td>0.633376</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.015000</td>\n",
              "      <td>0.649690</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.863059</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.097900</td>\n",
              "      <td>0.722403</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.701739</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.017300</td>\n",
              "      <td>0.710529</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.758333</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.015700</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>0.594142</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.007600</td>\n",
              "      <td>0.610352</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.007200</td>\n",
              "      <td>0.617437</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.006800</td>\n",
              "      <td>0.630026</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>0.642642</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>0.651258</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.655423</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.005700</td>\n",
              "      <td>0.666385</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.676334</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.683186</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>0.690118</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>0.697269</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>0.703414</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.711022</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.720255</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>0.724396</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.730874</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.734190</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.737953</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.742754</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.745459</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.747521</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.747575</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.750237</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.751830</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.753071</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.753599</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.753818</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'f1': 0.20000000000000004, 'roc_auc': 0.5, 'accuracy': 0.2}\n",
            "{'f1': 0.0, 'roc_auc': 0.5, 'accuracy': 0.0}\n",
            "{'f1': 0.0, 'roc_auc': 0.5, 'accuracy': 0.0}\n",
            "{'f1': 0.125, 'roc_auc': 0.5333333333333333, 'accuracy': 0.06666666666666667}\n",
            "{'f1': 0.2105263157894737, 'roc_auc': 0.55, 'accuracy': 0.13333333333333333}\n",
            "{'f1': 0.27272727272727276, 'roc_auc': 0.5666666666666667, 'accuracy': 0.13333333333333333}\n",
            "{'f1': 0.33333333333333337, 'roc_auc': 0.5916666666666666, 'accuracy': 0.26666666666666666}\n",
            "{'f1': 0.4444444444444445, 'roc_auc': 0.65, 'accuracy': 0.3333333333333333}\n",
            "{'f1': 0.4615384615384615, 'roc_auc': 0.6583333333333333, 'accuracy': 0.4}\n",
            "{'f1': 0.3571428571428571, 'roc_auc': 0.6, 'accuracy': 0.3333333333333333}\n",
            "{'f1': 0.5, 'roc_auc': 0.6833333333333333, 'accuracy': 0.4666666666666667}\n",
            "{'f1': 0.5517241379310344, 'roc_auc': 0.7166666666666666, 'accuracy': 0.5333333333333333}\n",
            "{'f1': 0.5, 'roc_auc': 0.6833333333333333, 'accuracy': 0.4666666666666667}\n",
            "{'f1': 0.5, 'roc_auc': 0.6833333333333333, 'accuracy': 0.4666666666666667}\n",
            "{'f1': 0.5517241379310344, 'roc_auc': 0.7166666666666666, 'accuracy': 0.5333333333333333}\n",
            "{'f1': 0.5517241379310344, 'roc_auc': 0.7166666666666666, 'accuracy': 0.5333333333333333}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.4666666666666667, 'roc_auc': 0.6666666666666667, 'accuracy': 0.4666666666666667}\n",
            "{'f1': 0.5517241379310344, 'roc_auc': 0.7166666666666666, 'accuracy': 0.5333333333333333}\n",
            "{'f1': 0.5517241379310344, 'roc_auc': 0.7166666666666666, 'accuracy': 0.5333333333333333}\n",
            "{'f1': 0.6206896551724138, 'roc_auc': 0.7583333333333334, 'accuracy': 0.6}\n",
            "{'f1': 0.6666666666666666, 'roc_auc': 0.7916666666666666, 'accuracy': 0.6666666666666666}\n",
            "{'f1': 0.6666666666666666, 'roc_auc': 0.7916666666666666, 'accuracy': 0.6666666666666666}\n",
            "{'f1': 0.6666666666666666, 'roc_auc': 0.7916666666666666, 'accuracy': 0.6666666666666666}\n",
            "{'f1': 0.6666666666666666, 'roc_auc': 0.7916666666666666, 'accuracy': 0.6666666666666666}\n",
            "{'f1': 0.6666666666666666, 'roc_auc': 0.7916666666666666, 'accuracy': 0.6666666666666666}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n",
            "{'f1': 0.6, 'roc_auc': 0.7500000000000001, 'accuracy': 0.6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=650, training_loss=0.099953117490961, metrics={'train_runtime': 1745.3837, 'train_samples_per_second': 3.466, 'train_steps_per_second': 0.372, 'total_flos': 2819120949427200.0, 'train_loss': 0.099953117490961, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}