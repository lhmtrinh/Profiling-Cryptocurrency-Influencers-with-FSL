{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from model import *\n",
    "\n",
    "\n",
    "RANDOM_SEED = 20\n",
    "NUMBER_LABELS = 5 \n",
    "NUMBER_FEATURES = 9\n",
    "MODEL = 'charlieoneill/distilbert-base-uncased-finetuned-tweet_eval-offensive'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"../../data/data_old_v2/processed_data_Q\"+str(Q_NO)+\".csv\").drop(['plagiarized_score', 'plagiarized_index'],axis=1)\n",
    "\n",
    "df = df[~df['ID'].isin(IDS)]\n",
    "\n",
    "# transform label\n",
    "df[VAR_S] = df[VAR_S]-1\n",
    "df[VAR_S].value_counts()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = pd.DataFrame(scaler.fit_transform(df.loc[:,'length_in_words':]))\n",
    "df = pd.concat([df.loc[:,['ID',VAR_Q,VAR_S]], features], axis=1)\n",
    "\n",
    "# define the downsampling condition\n",
    "condition =  f'{VAR_S} == 1 |  {VAR_S} == 2'\n",
    "\n",
    "# filter the dataframe based on the condition\n",
    "downsampled_df = df.query(condition)\n",
    "\n",
    "# randomly select a subset of rows to keep\n",
    "frac_to_keep = 0.5  # fraction of rows to keep\n",
    "downsampled_df = downsampled_df.sample(frac=frac_to_keep)\n",
    "\n",
    "df = df.query(f'{VAR_S} != 1 & {VAR_S} != 2')\n",
    "\n",
    "df = pd.concat([df,downsampled_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val, df_test = train_test_split(df, test_size=0.15,random_state=RANDOM_SEED)\n",
    "df_train, df_val = train_test_split(df_train_val,test_size=0.2,random_state=RANDOM_SEED)\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "df_val = df_val[df_val['ID']!=1].dropna()\n",
    "df_test = df_test[df_test['ID']!=1].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097\n",
      "248\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pytorch dataset class and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train.drop(columns=['ID']), tokenizer, 550, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val.drop(columns=['ID']), tokenizer, 550, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test.drop(columns=['ID']), tokenizer, 550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "model = LFG_grading(NUMBER_LABELS,NUMBER_FEATURES).to(device)\n",
    "model.requires_grad_embeddings(True)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = OrdinalLoss(weight=class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Q6.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model,f\"{NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(LFG_grading,NUMBER_LABELS,NUMBER_FEATURES,f\"{NAME_TO_SAVE}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7eae479ad7e0b273445e45fe91474bb526c045303adb1db655a8cfe79e544b35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
