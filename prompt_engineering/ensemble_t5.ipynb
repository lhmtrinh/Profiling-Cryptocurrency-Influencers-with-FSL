{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (4.29.0.dev0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: requests in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: filelock in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, ElectraTokenizer, ElectraModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = \"t5-small\"\n",
    "tokenizer_name_1 = \"t5-small\"\n",
    "model_size_1 = 256\n",
    "model_name_2 = 'google/electra-small-discriminator'\n",
    "model_size_2 = 256\n",
    "EPOCHS = 20\n",
    "max_tweet_len = 512\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBodyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model_1, model_2, model_1_size, model_2_size, num_labels):\n",
    "        super(TwoBodyModel, self).__init__()\n",
    "        # electra model has output size 256, distilbert has 768\n",
    "        self.num_labels = num_labels\n",
    "        self.model_1 = AutoModel.from_pretrained(model_1)\n",
    "        self.model_2 = AutoModel.from_pretrained(model_2)\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.pre_classifier_1 = nn.Linear(model_1_size, model_1_size)\n",
    "        self.pre_classifier_2 = nn.Linear(model_2_size, model_2_size)\n",
    "        self.dropout_1_2 = nn.Dropout(0.1)\n",
    "        self.dropout_2_2 = nn.Dropout(0.1)\n",
    "        self.classifier_1 = nn.Linear(model_1_size+model_2_size, model_1_size+model_2_size)\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "        self.classifier_2 = nn.Linear(model_1_size+model_2_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids_1, input_ids_2, attention_mask_1, attention_mask_2, labels):\n",
    "        #  model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        output_1 = self.model_1(input_ids=input_ids_1, attention_mask=attention_mask_1, decoder_input_ids = input_ids_1)\n",
    "        output_2 = self.model_2(input_ids=input_ids_2, attention_mask=attention_mask_2)\n",
    "        # print('output12 ', len(output_1), len(output_2))\n",
    "        # print('output1 ', output_1)\n",
    "        # print(output_1[0].shape)\n",
    "        # print(output_2[0].shape)\n",
    "        # use: [CLS] token, we can obtain it by typing outputs[0][:, 0, :]\n",
    "        pre_output_1 = self.dropout_1(F.gelu(output_1[0][:, 0, :]))\n",
    "        pre_output_2 = self.dropout_2(F.gelu(output_2[0][:, 0, :]))\n",
    "        # print('pre_output1', pre_output_1.shape)\n",
    "        # print('pre_output2', pre_output_2.shape)\n",
    "        pre_output_1 = self.pre_classifier_1(pre_output_1)\n",
    "        pre_output_1 = self.dropout_1_2(F.gelu(pre_output_1))\n",
    "        pre_output_2 = self.pre_classifier_2(pre_output_2)\n",
    "        pre_output_2 = self.dropout_2_2(F.gelu(pre_output_2))\n",
    "        output = self.classifier_1(torch.cat((pre_output_1, pre_output_2), 1))\n",
    "        # print('output', output.shape)\n",
    "        output = self.dropout_3(F.gelu(output))\n",
    "        logits = self.classifier_2(output)\n",
    "        # print('logits', logits.shape)\n",
    "        # print('labels', labels.shape)\n",
    "        aa = logits.view(-1, self.num_labels)\n",
    "        bb = labels.view(-1)\n",
    "        # print('aa', aa.shape)\n",
    "        # print('bb', bb.shape)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddeac3f0f245401b898778bb9f8608de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf51d28436bb4ab09c86f089e780be8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f434ba12d47fb9fde3817e4a04d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcfe1464b6842f3906c14a307279567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa27bce75da54ed98c61578c36e222df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ca5c11ddb34d39baa9994cced1656d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a5d9ba57dc47ec908d63723ac4e7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_1 = AutoTokenizer.from_pretrained(tokenizer_name_1)\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/finetune_train_val_test/train.csv')\n",
    "validate_df = pd.read_csv('../data/finetune_train_val_test/validate.csv')\n",
    "train_df = train_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()\n",
    "validate_df = validate_df.groupby('twitter user id').agg({'texts': ' '.join, 'class': 'first', 'count_mention': sum}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['nano', 'no influencer', 'macro', 'mega', 'micro'],\n",
       " {'nano': 0, 'no influencer': 1, 'macro': 2, 'mega': 3, 'micro': 4})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_df['class'].unique().tolist()\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, max_len, tweet_df):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tweets_dataset = tweet_df\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.tweets_dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets_dataset.iloc[idx]['texts']\n",
    "        label = self.tweets_dataset.iloc[idx]['class']\n",
    "        user_id = self.tweets_dataset.iloc[idx]['twitter user id']\n",
    "        label = label2id[label]\n",
    "        labels_matrix = np.zeros(5)\n",
    "        labels_matrix[label] = 1\n",
    "   \n",
    "        encoding = self.tokenizer(\n",
    "            text = tweet,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'tweet': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(labels_matrix, dtype=torch.float),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'user_id': user_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = TweetDataset(tokenizer_1, max_tweet_len, train_df)\n",
    "val_dataset_1 = TweetDataset(tokenizer_1, max_tweet_len, validate_df)\n",
    "train_dataset_2 = TweetDataset(tokenizer_2, max_tweet_len, train_df)\n",
    "val_dataset_2 = TweetDataset(tokenizer_2, max_tweet_len, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_1 = DataLoader(train_dataset_1, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_data_loader_1 = DataLoader(val_dataset_1, batch_size=16, shuffle=True, num_workers=4)\n",
    "train_data_loader_2 = DataLoader(train_dataset_2, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_data_loader_2 = DataLoader(val_dataset_2, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43328569344b4e76b46b577e9beb3eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cfd15961a34720b16af1d9324176ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcf4430551d4eadb836a45ecdfb4142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TwoBodyModel(model_name_1, model_name_2, model_size_1, model_size_2, 5)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader_1, data_loader_2, optimizer, device, scheduler):  # , n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    total_acc = 0\n",
    "    total_counter = 0\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    strt = time.time()\n",
    "    \n",
    "    for i, d in enumerate(zip(data_loader_1, data_loader_2)):\n",
    "        d_1, d_2 = d[0], d[1]\n",
    "        # input_ids_1 = d_1[\"input_ids\"].reshape(bs, max_tweet_len).to(device)\n",
    "        input_ids_1 = d_1[\"input_ids\"].reshape(d_1[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "        attention_mask_1 = d_1[\"attention_mask\"].to(device)\n",
    "        targets_1 = d_1[\"label\"].to(device)\n",
    "\n",
    "        # input_ids_2 = d_2[\"input_ids\"].reshape(bs, max_tweet_len).to(device)\n",
    "        input_ids_2 = d_2[\"input_ids\"].reshape(d_2[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "        attention_mask_2 = d_2[\"attention_mask\"].to(device)\n",
    "        targets_2 = d_2[\"label\"].to(device)\n",
    "\n",
    "        # assert targets_1 == targets_2, 'target labels of both data loaders should be the same'\n",
    "\n",
    "        # strt_output = time.time()\n",
    "        # outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels=targets)\n",
    "        # orward(self, input_ids_1, input_ids_2, attention_mask_1, attention_mask_2, targets):\n",
    "        outputs = model(input_ids_1=input_ids_1, input_ids_2=input_ids_2, attention_mask_1=attention_mask_1,\n",
    "                        attention_mask_2=attention_mask_2, labels=targets_1)\n",
    "        # output_times += time.time()-strt_output\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        _, prediction = torch.max(outputs[1], dim=1)\n",
    "        targets = torch.argmax(targets_1, dim=1)\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        accuracy = accuracy_score(targets, prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        total_acc += accuracy\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # strt_backward = time.time()\n",
    "        loss.backward()\n",
    "        # backward_time += time.time()-strt_backward\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # strt_opt_sched = time.time()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # opt_sched_time += time.time()-strt_opt_sched\n",
    "        counter += 1\n",
    "        total_counter += 1\n",
    "        \n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print('iteration ', i, '; acc: ', acc / counter)\n",
    "            warnings.warn('iteration '+str(i)+'; acc: '+str(acc/counter)+'; time: '+str(time.time()-strt)+' s')\n",
    "            acc = 0\n",
    "            '''\n",
    "            print('avg model output time: ', output_times / counter)\n",
    "            output_times = 0\n",
    "            print('avg backward time: ', backward_time / counter)\n",
    "            backward_time = 0\n",
    "            print('avg optimizer sched time: ', opt_sched_time / counter)\n",
    "            opt_sched_time = 0\n",
    "            '''\n",
    "            counter = 0\n",
    "            print('time: ', time.time()-strt, ' s')\n",
    "            strt = time.time()\n",
    "            # print('input ids 1 shape: ', d_1[\"input_ids\"].shape)\n",
    "            # print('input ids 2 shape: ', d_2[\"input_ids\"].shape)\n",
    "\n",
    "\n",
    "    return total_acc / total_counter, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader_1, data_loader_2, device):  #, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    preds = []\n",
    "    targets_array = []\n",
    "    logit_array = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(zip(data_loader_1, data_loader_2)):\n",
    "            d_1, d_2 = d[0], d[1]\n",
    "            input_ids_1 = d_1[\"input_ids\"].reshape(d_1[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "            attention_mask_1 = d_1[\"attention_mask\"].to(device)\n",
    "            targets_1 = d_1[\"label\"].to(device)\n",
    "\n",
    "            input_ids_2 = d_2[\"input_ids\"].reshape(d_2[\"input_ids\"].shape[0], max_tweet_len).to(device)\n",
    "            attention_mask_2 = d_2[\"attention_mask\"].to(device)\n",
    "            targets_2 = d_2[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids_1=input_ids_1, input_ids_2=input_ids_2, attention_mask_1=attention_mask_1, attention_mask_2=attention_mask_2, labels=targets_1)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "            _, prediction = torch.max(outputs[1], dim=1)\n",
    "            targets = torch.argmax(targets_1, dim=1)\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            prediction = prediction.cpu().detach().numpy()\n",
    "            accuracy = accuracy_score(targets, prediction)\n",
    "            preds += list(prediction.flatten())\n",
    "            targets_array += list(targets.flatten())\n",
    "\n",
    "            acc += accuracy\n",
    "            losses.append(loss.item())\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "            logit_array += list(logits.cpu().detach().numpy())\n",
    "\n",
    "    return acc / counter, np.mean(losses), preds, np.array(logit_array), targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/lehoangminhtrinh/env/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_data_loader_1) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.58 GiB total capacity; 13.64 GiB already allocated; 1.31 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m train_acc, train_loss \u001b[39m=\u001b[39m train_epoch(\n\u001b[1;32m     12\u001b[0m     model,\n\u001b[1;32m     13\u001b[0m     train_data_loader_1,\n\u001b[1;32m     14\u001b[0m     train_data_loader_2,\n\u001b[1;32m     15\u001b[0m     optimizer,\n\u001b[1;32m     16\u001b[0m     device,\n\u001b[1;32m     17\u001b[0m     scheduler\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain loss \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m Train accuracy \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m val_acc, val_loss, predictions, val_logits, targets_arr \u001b[39m=\u001b[39m eval_model(\n\u001b[1;32m     23\u001b[0m     model,\n\u001b[1;32m     24\u001b[0m     val_data_loader_1,\n\u001b[1;32m     25\u001b[0m     val_data_loader_2,\n\u001b[1;32m     26\u001b[0m     device\n\u001b[1;32m     27\u001b[0m )\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader_1, data_loader_2, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m targets_2 \u001b[39m=\u001b[39m d_2[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[39m# assert targets_1 == targets_2, 'target labels of both data loaders should be the same'\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[39m# strt_output = time.time()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels=targets)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# orward(self, input_ids_1, input_ids_2, attention_mask_1, attention_mask_2, targets):\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids_1\u001b[39m=\u001b[39;49minput_ids_1, input_ids_2\u001b[39m=\u001b[39;49minput_ids_2, attention_mask_1\u001b[39m=\u001b[39;49mattention_mask_1,\n\u001b[1;32m     28\u001b[0m                 attention_mask_2\u001b[39m=\u001b[39;49mattention_mask_2, labels\u001b[39m=\u001b[39;49mtargets_1)\n\u001b[1;32m     29\u001b[0m \u001b[39m# output_times += time.time()-strt_output\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mTwoBodyModel.forward\u001b[0;34m(self, input_ids_1, input_ids_2, attention_mask_1, attention_mask_2, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids_1, input_ids_2, attention_mask_1, attention_mask_2, labels):\n\u001b[1;32m     20\u001b[0m     \u001b[39m#  model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     output_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_1(input_ids\u001b[39m=\u001b[39minput_ids_1, attention_mask\u001b[39m=\u001b[39mattention_mask_1, decoder_input_ids \u001b[39m=\u001b[39m input_ids_1)\n\u001b[0;32m---> 22\u001b[0m     output_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_2(input_ids\u001b[39m=\u001b[39;49minput_ids_2, attention_mask\u001b[39m=\u001b[39;49mattention_mask_2)\n\u001b[1;32m     23\u001b[0m     \u001b[39m# print('output12 ', len(output_1), len(output_2))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39m# print('output1 ', output_1)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39m# print(output_1[0].shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m# print(output_2[0].shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m# use: [CLS] token, we can obtain it by typing outputs[0][:, 0, :]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     pre_output_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_1(F\u001b[39m.\u001b[39mgelu(output_1[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m, :]))\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py:919\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39membeddings_project\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    917\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings_project(hidden_states)\n\u001b[0;32m--> 919\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    920\u001b[0m     hidden_states,\n\u001b[1;32m    921\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    922\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    923\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    924\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    925\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    926\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    927\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    929\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    930\u001b[0m )\n\u001b[1;32m    932\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py:588\u001b[0m, in \u001b[0;36mElectraEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    580\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    581\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    586\u001b[0m     )\n\u001b[1;32m    587\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    589\u001b[0m         hidden_states,\n\u001b[1;32m    590\u001b[0m         attention_mask,\n\u001b[1;32m    591\u001b[0m         layer_head_mask,\n\u001b[1;32m    592\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    593\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    594\u001b[0m         past_key_value,\n\u001b[1;32m    595\u001b[0m         output_attentions,\n\u001b[1;32m    596\u001b[0m     )\n\u001b[1;32m    598\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py:472\u001b[0m, in \u001b[0;36mElectraLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    473\u001b[0m         hidden_states,\n\u001b[1;32m    474\u001b[0m         attention_mask,\n\u001b[1;32m    475\u001b[0m         head_mask,\n\u001b[1;32m    476\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    477\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py:399\u001b[0m, in \u001b[0;36mElectraAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 399\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    400\u001b[0m         hidden_states,\n\u001b[1;32m    401\u001b[0m         attention_mask,\n\u001b[1;32m    402\u001b[0m         head_mask,\n\u001b[1;32m    403\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    404\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    405\u001b[0m         past_key_value,\n\u001b[1;32m    406\u001b[0m         output_attentions,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    408\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    409\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py:337\u001b[0m, in \u001b[0;36mElectraSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    333\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[1;32m    335\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[0;32m--> 337\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[1;32m    338\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n\u001b[1;32m    339\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mview(new_context_layer_shape)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.58 GiB total capacity; 13.64 GiB already allocated; 1.31 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "real_preds = []\n",
    "tt = None\n",
    "pp = None\n",
    "vacc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader_1,\n",
    "        train_data_loader_2,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss, predictions, val_logits, targets_arr = eval_model(\n",
    "        model,\n",
    "        val_data_loader_1,\n",
    "        val_data_loader_2,\n",
    "        device\n",
    "    )\n",
    "    if val_acc > vacc:\n",
    "        tt = targets_arr\n",
    "        pp = predictions\n",
    "        vacc = val_acc\n",
    "\n",
    "    # _, _, real_predictions, real_logits = eval_model(\n",
    "    #     model,\n",
    "    #     real_data_loader_1,\n",
    "    #     real_data_loader_2,\n",
    "    #     device\n",
    "    # )\n",
    "\n",
    "    # warnings.warn('val loss: '+str(val_loss)+'; valuation acc: '+str(val_acc))\n",
    "\n",
    "    # print('tweets: ', X_test[:10])\n",
    "    # print('pred: ', predictions[:10])\n",
    "    # print('real: ', Y_test[:10])\n",
    "\n",
    "    # warnings.warn('logits shape: '+str(real_logits.shape))\n",
    "    # np.save('electra_distbert_sst2_gelu_'+str(tr)+'_'+str(epoch)+'_val_logits', val_logits)\n",
    "    # np.save('electra_distbert_sst2_gelu_'+str(tr)+'_'+str(epoch)+'_real_logits', real_logits)\n",
    "\n",
    "    # preds = [p if p==1 else -1 for p in predictions]\n",
    "    # real_preds = [p if p==1 else -1 for p in real_predictions]\n",
    "\n",
    "    # write test data to csv\n",
    "    # with open('electra_distbert_sst2_gelu_'+str(tr)+'_'+str(epoch)+'_predictions.csv', 'w', newline='') as f:\n",
    "    #     fieldnames = ['Id', 'Prediction']\n",
    "    #     writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "    #     writer.writeheader()\n",
    "    #     for j, p in enumerate(real_preds):\n",
    "    #         writer.writerow({'Id':j+1, 'Prediction':p})\n",
    "\n",
    "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
    "\n",
    "cm = confusion_matrix(tt, pp)\n",
    "print(classification_report(tt, pp))\n",
    "\n",
    "# Create a heatmap plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
